{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monty Hall Environment\n",
    "\n",
    "L'agent est un candidat au jeu Monty Hall, Il doit prendre 2 décisions successives. Dans cet \n",
    "environnement, il y a 3 portes A, B et C. Au démarrage de l'environnement, une porte est tirée au hasard \n",
    "de manière cachée pour l'agent, il s'agit de la porte gagnante. La première action de l'agent est de choisir \n",
    "une porte parmi les trois portes. Ensuite, une porte parmi les 2 restantes non choisies par l'agent est \n",
    "retirée du jeu, il s'agit forcément d'une porte non gagnante. L'agent ensuite doit effectuer une nouvelle \n",
    "action : choisir de conserver la porte choisie au départ ou changer pour la porte restante. Une fois le choix \n",
    "fait, la porte choisie est 'ouverte' et l'on découvre si elle était gagnante (reward de 1.0) ou non (reward de \n",
    "0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe MontyHall Level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from montyhall import MontyHallEnvLevel2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Naïf Q-learning\n",
    "\n",
    "L'algorithme naïf de Q-learning est une méthode d'apprentissage par renforcement sans modèle qui cherche à apprendre la politique optimale pour un agent en explorant et en exploitant un environnement. Cet algorithme utilise une table de valeurs Q pour stocker et mettre à jour les estimations des récompenses futures attendues pour chaque paire état-action. Voici les étapes clés de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La table Q est initialement remplie de valeurs arbitraires, et ces valeurs sont progressivement ajustées à travers les épisodes d'apprentissage.\n",
    "\n",
    "2. **Choix des actions** : À chaque étape, l'agent choisit une action basée soit sur l'exploration (choix aléatoire) pour découvrir de nouvelles stratégies, soit sur l'exploitation (choisir l'action avec la valeur Q la plus élevée pour l'état actuel) pour maximiser les récompenses.\n",
    "\n",
    "3. **Mise à jour de Q** : Après avoir pris une action, l'agent observe la récompense obtenue et le nouvel état atteint. La valeur Q pour la paire état-action est ensuite mise à jour en utilisant la formule :\n",
    "   \\[\n",
    "   Q(s, a) = Q(s, a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right]\n",
    "   \\]\n",
    "   où \\( \\alpha \\) est le taux d'apprentissage, \\( \\gamma \\) est le facteur de dépréciation, \\( r \\) est la récompense observée, \\( s' \\) est le nouvel état, et \\( a' \\) est la meilleure action possible dans le nouvel état.\n",
    "\n",
    "4. **Répétition** : Ces étapes sont répétées pour de nombreux épisodes jusqu'à ce que la table Q converge vers une approximation de la fonction de valeur optimale, permettant à l'agent de suivre la politique optimale déduite de Q.\n",
    "\n",
    "L'algorithme de Q-learning est particulièrement utile pour les problèmes avec un espace d'état et d'action discrets et peut être appliqué à une variété de tâches d'apprentissage et de décision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 55844.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 0}, {0: {0: 0.20017937531886448, 1: 0.04467919678750267}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_q_learning(env_type, alpha: float = 0.1, epsilon: float = 0.1, gamma: float = 0.999, nb_iter: int = 100000):\n",
    "    Q = {}\n",
    "\n",
    "    env = env_type()\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if s not in Q:\n",
    "                Q[s] = {}\n",
    "                for a in aa:\n",
    "                    Q[s][a] = np.random.random()  # Initialize Q-values for each state-action pair\n",
    "\n",
    "            # Decide whether to explore or exploit\n",
    "            if np.random.random() < epsilon:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                # Choose the best action based on current Q-values\n",
    "                q_s = [Q[s][a] for a in aa]\n",
    "                best_a_index = np.argmax(q_s)\n",
    "                a = aa[best_a_index]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            s_p = env.state_id()\n",
    "            aa_p = env.available_actions()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                target = r\n",
    "            else:\n",
    "                if s_p not in Q:\n",
    "                    Q[s_p] = {}\n",
    "                    for a_p in aa_p:\n",
    "                        Q[s_p][a_p] = np.random.random()\n",
    "                q_s_p = [Q[s_p][a_p] for a_p in aa_p]\n",
    "                max_a_p = np.max(q_s_p)\n",
    "                target = r + gamma * max_a_p\n",
    "\n",
    "            Q[s][a] = (1 - alpha) * Q[s][a] + alpha * target\n",
    "\n",
    "    # Extract policy from Q-values\n",
    "    Pi = {}\n",
    "    for s, actions in Q.items():\n",
    "        best_a = max(actions, key=actions.get)\n",
    "        Pi[s] = best_a\n",
    "\n",
    "    return Pi, Q\n",
    "\n",
    "# Example usage:\n",
    "env = MontyHallEnvLevel2()\n",
    "naive_q_learning(env_type=env, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Monte Carlo avec départs exploratoires (ES)\n",
    "\n",
    "L'algorithme de Monte Carlo avec départs exploratoires (ES) est une technique d'apprentissage par renforcement qui permet d'estimer la politique optimale en utilisant l'échantillonnage complet des retours (gains cumulés) de chaque épisode. Contrairement aux méthodes basées sur le temps différé comme Q-learning, les méthodes Monte Carlo ajustent les estimations de la politique uniquement à la fin de chaque épisode. Voici les étapes principales de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La politique (Pi) et la table de valeurs Q sont initialisées. Pour chaque état, une action est choisie aléatoirement comme action par défaut. Un dictionnaire de retours est également initialisé pour stocker les retours accumulés pour chaque paire état-action.\n",
    "\n",
    "2. **Départs exploratoires** : Chaque épisode commence dans un état initial choisi aléatoirement avec une action également choisie aléatoirement. Cela assure que toutes les paires état-action ont une chance d'être explorées suffisamment.\n",
    "\n",
    "3. **Génération de l'épisode** : L'agent suit la politique actuelle sauf pour le premier choix qui est aléatoire. L'épisode est enregistré sous forme de séquence de triplets (état, action, récompense).\n",
    "\n",
    "4. **Calcul du retour** : À la fin de l'épisode, le retour pour chaque étape est calculé en remontant depuis la fin de l'épisode jusqu'au début, en utilisant le facteur de dépréciation \\( \\gamma \\). Le retour est le gain cumulé à partir de cette étape jusqu'à la fin de l'épisode.\n",
    "\n",
    "5. **Mise à jour de Q et de la politique** :\n",
    "   - **Mise à jour de Q** : Pour chaque paire état-action unique rencontrée dans l'épisode, le retour est ajouté à la liste des retours correspondants et la valeur Q est mise à jour comme la moyenne de ces retours.\n",
    "   - **Mise à jour de la politique** : Pour chaque état visité, la politique est mise à jour pour choisir l'action qui maximise la valeur Q estimée pour cet état.\n",
    "\n",
    "6. **Répétition** : Le processus est répété pour un grand nombre d'épisodes pour permettre à la politique de converger vers la politique optimale.\n",
    "\n",
    "Cet algorithme est particulièrement utile dans les environnements avec des dynamiques simples où la méthode des départs exploratoires est faisable. Il est aussi bien adapté aux situations où il est possible de simuler l'environnement pour générer des épisodes avec des conditions de départ arbitraires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 70723.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def naive_monte_carlo_with_exploring_starts(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    Returns = defaultdict(list)\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        is_first_action = True\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "        \n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if is_first_action:\n",
    "                a = np.random.choice(aa)\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                if s not in Pi:\n",
    "                    Pi[s] = np.random.choice(aa)\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        for (t, (s, a, r, aa)) in reversed(list(enumerate(trajectory))):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if all(map(lambda triplet: triplet[0] != s or triplet[1] != a, trajectory[:t])):\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = None\n",
    "                best_a_score = float('-inf')\n",
    "                for a in aa:\n",
    "                    if (s, a) not in Q:\n",
    "                        Q[(s, a)] = np.random.random()\n",
    "                    if Q[(s, a)] > best_a_score:\n",
    "                        best_a = a\n",
    "                        best_a_score = Q[(s, a)]\n",
    "\n",
    "                Pi[s] = best_a\n",
    "\n",
    "    return Pi\n",
    "\n",
    "# Run the Monte Carlo ES algorithm\n",
    "optimal_policy_mc = naive_monte_carlo_with_exploring_starts(MontyHallEnvLevel2, nb_iter=10000)\n",
    "print(optimal_policy_mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme On-policy First visit Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 68350.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def on_policy_first_visit_mc_control(env_type, gamma=0.999, epsilon=0.1, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    Returns = defaultdict(list)\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if np.random.random() < epsilon or s not in Pi:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        visited_state_action_pairs = set()\n",
    "        for (s, a, r, aa) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if (s, a) not in visited_state_action_pairs:\n",
    "                visited_state_action_pairs.add((s, a))\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = max(aa, key=lambda action: Q[(s, action)])\n",
    "                Pi[s] = best_a\n",
    "\n",
    "    return Pi\n",
    "\n",
    "# Run the On-Policy First Visit MC Control algorithm\n",
    "optimal_policy_q = on_policy_first_visit_mc_control(MontyHallEnvLevel2, nb_iter=10000)\n",
    "print(optimal_policy_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off Policy First visit Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 73883.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def off_policy_mc_control(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    C = defaultdict(lambda: 0.0)\n",
    "    b = defaultdict(lambda: 1.0)  # behavior policy (random)\n",
    "\n",
    "    Pi = {}\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            a = np.random.choice(aa)\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for (s, a, r) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "            \n",
    "            Pi[s] = max(env.available_actions(), key=lambda action: Q[(s, action)])\n",
    "            \n",
    "            if a != Pi[s]:\n",
    "                break\n",
    "            \n",
    "            W *= 1.0 / b[(s, a)]\n",
    "\n",
    "    return Pi\n",
    "\n",
    "off_policy_mc_control(env, nb_iter=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Number of episodes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create the environment and run the SARSA algorithm\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMontyHallEnvLevel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Display the optimal policy found\u001b[39;00m\n\u001b[1;32m     54\u001b[0m env \u001b[38;5;241m=\u001b[39m MontyHallEnvLevel2()\n",
      "Cell \u001b[0;32mIn[36], line 24\u001b[0m, in \u001b[0;36msarsa\u001b[0;34m(env_class, num_episodes, alpha, gamma, epsilon)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m     23\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 24\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39mavailable_actions()) \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m epsilon \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(state)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (1,25)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "def sarsa(env_class, num_episodes, alpha, gamma, epsilon):\n",
    "    env = env_class()\n",
    "    state_size = env.num_states()\n",
    "    action_size = env.num_actions()\n",
    "    model = build_model(state_size, action_size)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        action = np.random.choice(env.available_actions()) if np.random.rand() < epsilon else np.argmax(model.predict(state)[0])\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            next_action = np.random.choice(env.available_actions()) if np.random.rand() < epsilon else np.argmax(model.predict(next_state)[0])\n",
    "\n",
    "            target = reward + gamma * model.predict(next_state)[0][next_action] if not done else reward\n",
    "            target_f = model.predict(state)\n",
    "            target_f[0][action] = (1 - alpha) * target_f[0][action] + alpha * target\n",
    "\n",
    "            model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parameters for SARSA\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes\n",
    "\n",
    "# Create the environment and run the SARSA algorithm\n",
    "model = sarsa(MontyHallEnvLevel2, num_episodes, alpha, gamma, epsilon)\n",
    "\n",
    "# Display the optimal policy found\n",
    "env = MontyHallEnvLevel2()\n",
    "optimal_policy = []\n",
    "\n",
    "for state in range(env.num_states()):\n",
    "    state_input = np.reshape(state, [1, env.num_states()])\n",
    "    optimal_action = np.argmax(model.predict(state_input)[0])\n",
    "    optimal_policy.append(optimal_action)\n",
    "    print(f\"State {state}: Optimal Action {optimal_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "47",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m planning_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of planning steps\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Create the environment and run the Dyna-Q algorithm\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m Q \u001b[38;5;241m=\u001b[39m dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Display the optimal policy found\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mnum_states()):\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mdyna_q\u001b[0;34m(env, num_episodes, alpha, gamma, epsilon, planning_steps)\u001b[0m\n\u001b[1;32m     21\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     22\u001b[0m next_available_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mavailable_actions()\n\u001b[0;32m---> 23\u001b[0m next_action \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(Q, next_state, epsilon, next_available_actions)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Update Q\u001b[39;00m\n\u001b[1;32m     26\u001b[0m Q[state][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m Q[next_state][next_action] \u001b[38;5;241m-\u001b[39m Q[state][action])\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(Q, state, epsilon, available_actions)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(available_actions)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(available_actions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Q[state][x])\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mepsilon_greedy_policy.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(available_actions)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(available_actions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Q[state][x])\n",
      "\u001b[0;31mKeyError\u001b[0m: 47"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon, available_actions):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(available_actions)\n",
    "    else:\n",
    "        return max(available_actions, key=lambda x: Q[state][x])\n",
    "\n",
    "def dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps):\n",
    "    Q = {state: [0.0, 0.0] for state in range(env.num_states())}  # Only 2 actions: stick (0) or switch (1)\n",
    "    model = {state: {} for state in range(env.num_states())}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.state_id()\n",
    "        available_actions = env.available_actions()\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, available_actions)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_available_actions = env.available_actions()\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, next_available_actions)\n",
    "\n",
    "            # Update Q\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "            # Update the model\n",
    "            model[state][action] = (reward, next_state)\n",
    "\n",
    "            # Planning\n",
    "            for _ in range(planning_steps):\n",
    "                sampled_state = random.choice(list(model.keys()))\n",
    "                if not model[sampled_state]:\n",
    "                    continue\n",
    "                sampled_action = random.choice(list(model[sampled_state].keys()))\n",
    "                sampled_reward, sampled_next_state = model[sampled_state][sampled_action]\n",
    "\n",
    "                Q[sampled_state][sampled_action] += alpha * (sampled_reward + gamma * max(Q[sampled_next_state]) - Q[sampled_state][sampled_action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes\n",
    "planning_steps = 5  # Number of planning steps\n",
    "\n",
    "# Create the environment and run the Dyna-Q algorithm\n",
    "Q = dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps)\n",
    "\n",
    "# Display the optimal policy found\n",
    "for state in range(env.num_states()):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    action_symbols = ['Stick', 'Switch']\n",
    "    print(f\"State {state}: {action_symbols[best_action]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy et Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env_class, gamma=0.99, theta=1e-3):\n",
    "        self.env_class = env_class\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.env = env_class()\n",
    "        self.num_states = self.env.num_states()\n",
    "        self.num_actions = self.env.num_actions()\n",
    "\n",
    "    def run(self) -> Tuple[List[int], List[float]]:\n",
    "        policy = np.random.choice(self.num_actions, size=self.num_states)\n",
    "        V = np.zeros(self.num_states)\n",
    "\n",
    "        while True:\n",
    "            V = self.policy_evaluation(policy, V)\n",
    "            policy_stable = self.policy_improvement(policy, V)\n",
    "            if policy_stable:\n",
    "                break\n",
    "\n",
    "        return policy, V\n",
    "\n",
    "    def policy_evaluation(self, policy, V):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "                v = V[s]\n",
    "                new_v = 0\n",
    "                for a in range(self.num_actions):\n",
    "                    if policy[s] == a:\n",
    "                        for next_state in range(self.num_states):\n",
    "                            for r_index in range(self.env.num_rewards()):\n",
    "                                prob = self.env.p(s, a, next_state, r_index)\n",
    "                                reward = self.env.reward(r_index)\n",
    "                                new_v += prob * (reward + self.gamma * V[next_state])\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "                V[s] = new_v\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(self, policy, V):\n",
    "        policy_stable = True\n",
    "        for s in range(self.num_states):\n",
    "            chosen_a = policy[s]\n",
    "            action_values = self.one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = best_a\n",
    "        return policy_stable\n",
    "\n",
    "    def one_step_lookahead(self, state, V):\n",
    "        A = np.zeros(self.num_actions)\n",
    "        for a in range(self.num_actions):\n",
    "            for next_state in range(self.num_states):\n",
    "                for r_index in range(self.env.num_rewards()):\n",
    "                    prob = self.env.p(state, a, next_state, r_index)\n",
    "                    reward = self.env.reward(r_index)\n",
    "                    A[a] += prob * (reward + self.gamma * V[next_state])\n",
    "        return A\n",
    "\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env_class, gamma=0.99, theta=1e-3):\n",
    "        self.env_class = env_class\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.env = env_class()\n",
    "        self.num_states = self.env.num_states()\n",
    "        self.num_actions = self.env.num_actions()\n",
    "\n",
    "    def run(self) -> List[float]:\n",
    "        V = np.zeros(self.num_states)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "                v = V[s]\n",
    "                action_values = self.one_step_lookahead(s, V)\n",
    "                V[s] = max(action_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def one_step_lookahead(self, state, V):\n",
    "        A = np.zeros(self.num_actions)\n",
    "        for a in range(self.num_actions):\n",
    "            for next_state in range(self.num_states):\n",
    "                for r_index in range(self.env.num_rewards()):\n",
    "                    prob = self.env.p(state, a, next_state, r_index)\n",
    "                    reward = self.env.reward(r_index)\n",
    "                    A[a] += prob * (reward + self.gamma * V[next_state])\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Stick\n",
      "State 1: Stick\n",
      "State 2: Stick\n",
      "State 3: Stick\n",
      "State 4: Stick\n",
      "State 5: Stick\n",
      "State 6: Stick\n",
      "State 7: Stick\n",
      "State 8: Stick\n",
      "State 9: Stick\n",
      "State 10: Stick\n",
      "State 11: Stick\n",
      "State 12: Stick\n",
      "State 13: Stick\n",
      "State 14: Stick\n",
      "State 15: Stick\n",
      "State 16: Stick\n",
      "State 17: Stick\n",
      "State 18: Stick\n",
      "State 19: Stick\n",
      "State 20: Stick\n",
      "State 21: Stick\n",
      "State 22: Stick\n",
      "State 23: Stick\n",
      "State 24: Stick\n",
      "State 25: Stick\n",
      "State 26: Stick\n",
      "State 27: Stick\n",
      "State 28: Stick\n",
      "State 29: Stick\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_class = MontyHallEnvLevel2()\n",
    "\n",
    "# Run Policy Iteration\n",
    "pi = PolicyIteration(env_class)\n",
    "policy, V = pi.run()\n",
    "\n",
    "# Display the optimal policy found\n",
    "action_symbols = ['Stick', 'Switch']\n",
    "for state in range(env_class().num_states()):\n",
    "    print(f\"State {state}: {action_symbols[policy[state]]}\")\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Value 0.0\n",
      "State 1: Value 0.0\n",
      "State 2: Value 0.0\n",
      "State 3: Value 0.0\n",
      "State 4: Value 0.0\n",
      "State 5: Value 0.0\n",
      "State 6: Value 0.0\n",
      "State 7: Value 0.0\n",
      "State 8: Value 0.0\n",
      "State 9: Value 0.0\n",
      "State 10: Value 0.0\n",
      "State 11: Value 0.0\n",
      "State 12: Value 0.0\n",
      "State 13: Value 0.0\n",
      "State 14: Value 0.0\n",
      "State 15: Value 0.0\n",
      "State 16: Value 0.0\n",
      "State 17: Value 0.0\n",
      "State 18: Value 0.0\n",
      "State 19: Value 0.0\n",
      "State 20: Value 0.0\n",
      "State 21: Value 0.0\n",
      "State 22: Value 0.0\n",
      "State 23: Value 0.0\n",
      "State 24: Value 0.0\n",
      "State 25: Value 0.0\n",
      "State 26: Value 0.0\n",
      "State 27: Value 0.0\n",
      "State 28: Value 0.0\n",
      "State 29: Value 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "\n",
    "\n",
    "# Run Value Iteration\n",
    "vi = ValueIteration(env_class)\n",
    "V = vi.run()\n",
    "\n",
    "# Display the value function found\n",
    "for state in range(env_class().num_states()):\n",
    "    print(f\"State {state}: Value {V[state]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FrameworkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
