{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "\n",
    "from montyhall import MontyHallEnvLevel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MontyHallEnvLevel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.15893185e+275 -9.99100360e+039]\n",
      " [ 2.23068663e-316  2.12199580e-314]\n",
      " [ 0.00000000e+000  4.94065646e-324]\n",
      " [ 4.40639758e-317  2.22976174e-316]\n",
      " [ 2.23068663e-316  0.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000]\n",
      " [ 2.12199579e-314  1.69759663e-311]\n",
      " [ 4.94065646e-324  2.23069374e-316]\n",
      " [ 2.23068940e-316  1.27319753e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 0.00000000e+000  1.06099795e-313]\n",
      " [ 0.00000000e+000  8.48798318e-314]\n",
      " [ 2.23069414e-316  5.78056806e-321]\n",
      " [ 0.00000000e+000  8.91238232e-313]\n",
      " [ 2.23069572e-316  5.54341655e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23069730e-316  6.36598793e-314]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23069888e-316  5.09278995e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070046e-316  1.48539711e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070204e-316  5.56317917e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23070362e-316  0.00000000e+000]]\n",
      "Episode 1: Total Reward: 0, Exploration Rate: 0.995\n",
      "Episode 2: Total Reward: 0, Exploration Rate: 0.990025\n",
      "Episode 3: Total Reward: 0, Exploration Rate: 0.985075\n",
      "Episode 4: Total Reward: 0, Exploration Rate: 0.98015\n",
      "Episode 5: Total Reward: 0, Exploration Rate: 0.975249\n",
      "Episode 6: Total Reward: 1, Exploration Rate: 0.970373\n",
      "Episode 7: Total Reward: 1, Exploration Rate: 0.965521\n",
      "Episode 8: Total Reward: 0, Exploration Rate: 0.960693\n",
      "Episode 9: Total Reward: 0, Exploration Rate: 0.95589\n",
      "Episode 10: Total Reward: 0, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "q_learning = lib.QLearning(env, 10000)\n",
    "q_table = q_learning.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.1892}\n"
     ]
    }
   ],
   "source": [
    "monte_carlo_es = lib.MonteCarloES(env, 10000)\n",
    "value_table = monte_carlo_es.run()\n",
    "print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {1: 0.2001622388967756, 0: 0.2004340106529888}}\n"
     ]
    }
   ],
   "source": [
    "mcoffpolicy = lib.OffPolicyMonteCarloControl(env, 10000)\n",
    "value_table = mcoffpolicy.run()\n",
    "print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {1: 0.17886178861788618, 0: 0.19867480016827935}}\n"
     ]
    }
   ],
   "source": [
    "onpolicy_mc_control = lib.onPolicyMonteCarloControl(env, 10000)\n",
    "q_table = onpolicy_mc_control.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.40811534e+186 -1.76236224e-026]\n",
      " [ 2.23068663e-316  2.12199580e-314]\n",
      " [ 0.00000000e+000  4.94065646e-324]\n",
      " [ 4.40639758e-317  2.22976174e-316]\n",
      " [ 2.23068663e-316  0.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000]\n",
      " [ 2.12199579e-314  1.69759663e-311]\n",
      " [ 4.94065646e-324  2.23069374e-316]\n",
      " [ 2.23068940e-316  1.27319753e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 0.00000000e+000  1.06099795e-313]\n",
      " [ 0.00000000e+000  8.48798318e-314]\n",
      " [ 2.23069414e-316  5.78056806e-321]\n",
      " [ 0.00000000e+000  8.91238232e-313]\n",
      " [ 2.23069572e-316  5.54341655e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23069730e-316  6.36598793e-314]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23069888e-316  5.09278995e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070046e-316  1.48539711e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070204e-316  5.56317917e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23070362e-316  0.00000000e+000]]\n",
      "Episode 1: Total Reward: 0, Exploration Rate: 0.995\n",
      "Episode 2: Total Reward: 0, Exploration Rate: 0.990025\n",
      "Episode 3: Total Reward: 1, Exploration Rate: 0.985075\n",
      "Episode 4: Total Reward: 0, Exploration Rate: 0.98015\n",
      "Episode 5: Total Reward: 0, Exploration Rate: 0.975249\n",
      "Episode 6: Total Reward: 0, Exploration Rate: 0.970373\n",
      "Episode 7: Total Reward: 0, Exploration Rate: 0.965521\n",
      "Episode 8: Total Reward: 0, Exploration Rate: 0.960693\n",
      "Episode 9: Total Reward: 0, Exploration Rate: 0.95589\n",
      "Episode 10: Total Reward: 0, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "dyna_Q = lib.DynaQ(env, 10000)\n",
    "q_table = dyna_Q.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table = [[-2.14788272e+267  9.99800010e+011]\n",
      " [ 2.23068663e-316  2.12199580e-314]\n",
      " [ 0.00000000e+000  4.94065646e-324]\n",
      " [ 4.40639758e-317  2.22976174e-316]\n",
      " [ 2.23068663e-316  0.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000]\n",
      " [ 2.12199579e-314  1.69759663e-311]\n",
      " [ 4.94065646e-324  2.23069374e-316]\n",
      " [ 2.23068940e-316  1.27319753e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 0.00000000e+000  1.06099795e-313]\n",
      " [ 0.00000000e+000  8.48798318e-314]\n",
      " [ 2.23069414e-316  5.78056806e-321]\n",
      " [ 0.00000000e+000  8.91238232e-313]\n",
      " [ 2.23069572e-316  5.54341655e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23069730e-316  6.36598793e-314]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23069888e-316  5.09278995e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070046e-316  1.48539711e-313]\n",
      " [ 0.00000000e+000  1.23516411e-322]\n",
      " [ 2.23070204e-316  5.56317917e-321]\n",
      " [ 0.00000000e+000  1.27319748e-313]\n",
      " [ 2.23070362e-316  0.00000000e+000]]\n",
      "State: 0, Action: 1, Reward: 1, Next State: 335, Next Action: 1\n",
      "Q(0, 1) updated to: 10000\n",
      "Episode 1: Total Reward: 1, Exploration Rate: 0.995\n",
      "State: 0, Action: 1, Reward: 0, Next State: 335, Next Action: 0\n",
      "Q(0, 1) updated to: -9.999e+07\n",
      "Episode 2: Total Reward: 0, Exploration Rate: 0.990025\n",
      "State: 0, Action: 0, Reward: 0, Next State: 211, Next Action: 0\n",
      "Q(0, 0) updated to: 2.40787e+190\n",
      "Episode 3: Total Reward: 0, Exploration Rate: 0.985075\n",
      "State: 0, Action: 0, Reward: 1, Next State: 241, Next Action: 0\n",
      "Q(0, 0) updated to: 2.16044e+247\n",
      "Episode 4: Total Reward: 1, Exploration Rate: 0.98015\n",
      "State: 0, Action: 0, Reward: 1, Next State: 242, Next Action: 1\n",
      "Q(0, 0) updated to: -2.16023e+251\n",
      "Episode 5: Total Reward: 1, Exploration Rate: 0.975249\n",
      "State: 0, Action: 0, Reward: 0, Next State: 236, Next Action: 0\n",
      "Q(0, 0) updated to: 2.16001e+255\n",
      "Episode 6: Total Reward: 0, Exploration Rate: 0.970373\n",
      "State: 0, Action: 0, Reward: 0, Next State: 242, Next Action: 0\n",
      "Q(0, 0) updated to: -2.14831e+259\n",
      "Episode 7: Total Reward: 0, Exploration Rate: 0.965521\n",
      "State: 0, Action: 0, Reward: 0, Next State: 236, Next Action: 0\n",
      "Q(0, 0) updated to: 2.1481e+263\n",
      "Episode 8: Total Reward: 0, Exploration Rate: 0.960693\n",
      "State: 0, Action: 0, Reward: 0, Next State: 241, Next Action: 1\n",
      "Q(0, 0) updated to: -2.14788e+267\n",
      "Episode 9: Total Reward: 0, Exploration Rate: 0.95589\n",
      "State: 0, Action: 1, Reward: 0, Next State: 365, Next Action: 0\n",
      "Q(0, 1) updated to: 9.998e+11\n",
      "Episode 10: Total Reward: 0, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "sarsa = lib.SARSA(env, 10000)\n",
    "q_table = sarsa.run()\n",
    "print(f'q_table = {q_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "policy_iteration = lib.PolicyIteration(env)\n",
    "policy, V = policy_iteration.run()\n",
    "print(policy)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State number 0 = 0.0\n",
      "State number 1 = 0.0\n",
      "State number 2 = 0.0\n",
      "State number 3 = 0.0\n",
      "State number 4 = 0.0\n",
      "State number 5 = 0.0\n",
      "State number 6 = 0.0\n",
      "State number 7 = 0.0\n",
      "State number 8 = 0.0\n",
      "State number 9 = 0.0\n",
      "State number 10 = 0.0\n",
      "State number 11 = 0.0\n",
      "State number 12 = 0.0\n",
      "State number 13 = 0.0\n",
      "State number 14 = 0.0\n",
      "State number 15 = 0.0\n",
      "State number 16 = 0.0\n",
      "State number 17 = 0.0\n",
      "State number 18 = 0.0\n",
      "State number 19 = 0.0\n",
      "State number 20 = 0.0\n",
      "State number 21 = 0.0\n",
      "State number 22 = 0.0\n",
      "State number 23 = 0.0\n",
      "State number 24 = 0.0\n"
     ]
    }
   ],
   "source": [
    "value_iteration = lib.ValueIteration(env)\n",
    "value = value_iteration.run()\n",
    "for i in range(len(value)):\n",
    "    print(f'State number {i} = {value[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
