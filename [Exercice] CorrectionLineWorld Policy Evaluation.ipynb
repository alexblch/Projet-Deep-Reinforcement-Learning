{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "beryQv5c6FTP"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19mv7NrhVZzx"
      },
      "source": [
        "#Définition d'un line world sous la forme d'un MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "psNAv57nOCMW"
      },
      "outputs": [],
      "source": [
        "S = [0, 1, 2 ,3, 4]\n",
        "A = [0, 1] # left right\n",
        "R = [-1, 0, 1]\n",
        "T = [0, 4]\n",
        "p = np.zeros((len(S), len(A), len(S), len(R))) # S, A, S_p, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "tGlcGlTA6Tex"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 1. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0.]\n",
            "   [0. 1. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 1. 0.]\n",
            "   [0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 1. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 1.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]\n",
            "   [0. 0. 0.]]]]\n"
          ]
        }
      ],
      "source": [
        "for s in range(len(S)):\n",
        "  for a in range(len(A)):\n",
        "    for s_p in range(len(S)):\n",
        "      for r in range(len(R)):\n",
        "        if s_p == s + 1 and a == 1 and R[r] == 0 and s in [1, 2]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "        if s_p == s - 1 and a == 0 and R[r] == 0 and s in [2, 3]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "p[3, 1, 4, 2] = 1.0\n",
        "p[1, 0, 0, 0] = 1.0\n",
        "\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnLQGzt4Vzxp"
      },
      "source": [
        "# Exercice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s43r9pUuWPtv"
      },
      "source": [
        "Implémentez une fonction prenant en paramètre une policy (un tableau numpy)\n",
        "et renvoyant la value de cette policy (un tableau numpy) pour chaque état du line world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "_96znS5JWF9H"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy : np.ndarray, V: np.ndarray = None) -> np.ndarray:\n",
        "  theta = 0.00001\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  gamma = 0.999\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(len(S)):\n",
        "      v = V[s]\n",
        "      total = 0\n",
        "      for a in range(len(A)):\n",
        "        pi_s_a = policy[s, a]\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += pi_s_a * p[s, a, s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "      V[s] = total\n",
        "      delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "    if delta < theta:\n",
        "      return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29NHJWr3WTnW"
      },
      "source": [
        "Définissez une policy jouant tout le temps à droite.\n",
        "Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB8Gd0vPWiue",
        "outputId": "72ead850-6226-494a-816b-ed0ff637e9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.       0.998001 0.999    1.       0.      ]\n"
          ]
        }
      ],
      "source": [
        "pi_right = np.array([[0.0, 1.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_right))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIKxQ_fW1wC"
      },
      "source": [
        "Définissez une policy jouant tout le temps à gauche. Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-_ZCGmRW31Q",
        "outputId": "eddb5fdf-2017-4855-e711-d3071d1034dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.       -1.       -0.999    -0.998001  0.      ]\n"
          ]
        }
      ],
      "source": [
        "pi_left = np.array([[1.0, 0.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_left))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_8nQtoBXJai"
      },
      "source": [
        "Définissez une policy uniformément aléatoire (50% de chance d'aller à gauche et 50% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUYtT2hYW_Fe",
        "outputId": "55d1ad5b-ecff-478f-f031-4f4e78760b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00000000e+00 -4.99992999e-01  6.99360232e-06  5.00003493e-01\n",
            "  0.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "pi_random = np.array([[0.5, 0.5] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_random))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQpwh-x_XXGN"
      },
      "source": [
        "Définissez une policy uniformément aléatoire (15% de chance d'aller à gauche et 85% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-HkR7HCXc2b",
        "outputId": "dc51aa3e-5959-4b6d-9773-c9cac2875ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.64651341 0.93801507 0.99056156 0.        ]\n"
          ]
        }
      ],
      "source": [
        "pi_weird_random = np.array([[0.15, 0.85] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_weird_random))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "nqSUVgCIis1z"
      },
      "outputs": [],
      "source": [
        "def naive_ineficient_policy_iteration():\n",
        "  theta = 0.000001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "\n",
        "  Pi = np.random.choice(A, len(S))\n",
        "\n",
        "  while True:\n",
        "    # Policy Evaluation\n",
        "    while True:\n",
        "      delta = 0.0\n",
        "\n",
        "      for s in S:\n",
        "        v = V[s]\n",
        "\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        V[s] = total\n",
        "\n",
        "        delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      best_a = None\n",
        "      best_action_score = -9999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if best_a is None or total >= best_action_score:\n",
        "          best_a = a\n",
        "          best_action_score = total\n",
        "\n",
        "      Pi[s] = best_a\n",
        "      if Pi[s] != old_action:\n",
        "        policy_stable = False\n",
        "\n",
        "    if policy_stable:\n",
        "      return Pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVMOCNl0j3Q-",
        "outputId": "ec4b0b57-c253-40a5-d823-0663e0efbf6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_ineficient_policy_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZshaI9mXiaF"
      },
      "source": [
        "# Définissez une variante d'un GRID WORLD sous la forme d'un MDP et évaluez différentes stratégies sur cette environnement.\n",
        "\n",
        "Le grid world est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle l'agent peut évoluer, il commence généralement sur la première ligne, première colonne. L'agent possède 4 actions possibles (gauche, droite, haut, bas). Si jamais l'agent atteint la dernière case de la première ligne => état terminal avec reward de -3. Si jamais l'agent atteint la dernière case de la dernière ligne => état terminal avec reward de 1. Si l'agent essaye de se déplacer en dehors des bords de la grille => état terminal avec score de -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "l_eC0-d9Xh7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world size\n",
        "S = np.zeros((5, 5))\n",
        "\n",
        "# Define the possible actions, order: [up, left, down, right]\n",
        "A = np.array([[-1, 0], [0, -1], [1, 0], [0, 1]])\n",
        "\n",
        "# Define the rewards for terminal states\n",
        "R = {(0, 4): -3, (4, 4): 1}\n",
        "\n",
        "# Define the transition function\n",
        "def transition(state, action):\n",
        "    row, col = state\n",
        "    new_row, new_col = row + action[0], col + action[1]\n",
        "\n",
        "    # Check for out of bounds - returning to the same state with a reward of -1\n",
        "    if new_row < 0 or new_row >= len(S) or new_col < 0 or new_col >= len(S):\n",
        "        return state, -1, True\n",
        "\n",
        "    # Check for the specific terminal states with custom rewards\n",
        "    if (new_row, new_col) in R:\n",
        "        return (new_row, new_col), R[(new_row, new_col)], True\n",
        "\n",
        "    # If the new state is not terminal, return the new state, a reward of 0, and False for non-terminal\n",
        "    return (new_row, new_col), 0, False\n",
        "\n",
        "# Initialize the transition probability matrix, correct dimensions this time\n",
        "p = np.zeros((len(S), len(S[0]), len(A), len(S), len(S[0])))\n",
        "\n",
        "# Fill in the transition probability matrix\n",
        "for row in range(len(S)):\n",
        "    for col in range(len(S[0])):\n",
        "        for action_idx, action in enumerate(A):\n",
        "            next_state, reward, done = transition((row, col), action)\n",
        "            if done:\n",
        "                # Assign a probability of 1 to remain in or move to the terminal state\n",
        "                p[row, col, action_idx, next_state[0], next_state[1]] = 1\n",
        "            else:\n",
        "                # Assign a probability of 1 to move to the next valid state\n",
        "                p[row, col, action_idx, next_state[0], next_state[1]] = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZTA1y6YwkJ"
      },
      "source": [
        "# Proposez plusieurs stratégies et évaluez les à l'aide de policy évaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Ve8dvoONYuq5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Evaluation for Right:\n",
            "[-2990.99901468 -2993.99301768 -2996.99001768 -2999.99001768\n",
            " -2999.99001768     0.             0.             0.\n",
            "     0.             0.             0.             0.\n",
            "     0.             0.             0.             0.\n",
            "     0.             0.             0.             0.\n",
            "   996.99967156   997.99767256   998.99667256   999.99667256\n",
            "   999.99667256]\n",
            "Policy Evaluation for Left:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "Policy Evaluation for Up:\n",
            "[    0.             0.             0.             0.\n",
            " -2999.99001768     0.             0.             0.\n",
            "     0.         -2999.99002767     0.             0.\n",
            "     0.             0.         -2996.99003764     0.\n",
            "     0.             0.             0.         -2993.9930476\n",
            "     0.             0.             0.             0.\n",
            " -2990.99905455]\n",
            "Policy Evaluation for Down:\n",
            "[  0.           0.           0.           0.         996.99301731\n",
            "   0.           0.           0.           0.         997.99101831\n",
            "   0.           0.           0.           0.         998.99001831\n",
            "   0.           0.           0.           0.         999.99001831\n",
            "   0.           0.           0.           0.         999.99001831]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world size\n",
        "S = np.zeros((5, 5))\n",
        "\n",
        "# Define possible actions, order: [up, left, down, right]\n",
        "A = np.array([[-1, 0], [0, -1], [1, 0], [0, 1]])\n",
        "\n",
        "# Define the rewards for terminal states\n",
        "R = {(0, 4): -3, (4, 4): 1}\n",
        "\n",
        "def index_to_pos(index):\n",
        "    \"\"\" Convert flat index to grid position \"\"\"\n",
        "    return (index // 5, index % 5)\n",
        "\n",
        "def pos_to_index(row, col):\n",
        "    \"\"\" Convert grid position to flat index \"\"\"\n",
        "    return row * 5 + col\n",
        "\n",
        "# Example transition probabilities (simplified)\n",
        "p = np.zeros((25, 4, 25))  # s, a, s'\n",
        "\n",
        "# Simplified transition model assuming deterministic outcomes\n",
        "for s in range(25):\n",
        "    pos = index_to_pos(s)\n",
        "    for a in range(4):\n",
        "        new_pos = (pos[0] + A[a, 0], pos[1] + A[a, 1])\n",
        "        if 0 <= new_pos[0] < 5 and 0 <= new_pos[1] < 5:\n",
        "            s_p = pos_to_index(*new_pos)\n",
        "        else:\n",
        "            s_p = s  # remains in the same state if out of bounds\n",
        "        p[s, a, s_p] = 1\n",
        "\n",
        "def policy_evaluation(policy, V=None):\n",
        "    theta = 0.00001\n",
        "    gamma = 0.999\n",
        "\n",
        "    if V is None:\n",
        "        V = np.zeros(25)  # Match number of states\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(25):\n",
        "            v = V[s]\n",
        "            total = 0\n",
        "            for a in range(4):\n",
        "                pi_s_a = policy[s, a]\n",
        "                for s_p in range(25):\n",
        "                    pos = index_to_pos(s_p)\n",
        "                    reward = R.get(pos, 0)\n",
        "                    total += pi_s_a * p[s, a, s_p] * (reward + gamma * V[s_p])\n",
        "            V[s] = total\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            return V\n",
        "\n",
        "# Define policies\n",
        "pi_right = np.zeros((25, 4))\n",
        "pi_right[:, 3] = 1  # Assume right is the 3rd action\n",
        "\n",
        "pi_left = np.zeros((25, 4))\n",
        "pi_left[:, 1] = 1  # Assume left is the 1st action\n",
        "\n",
        "pi_up = np.zeros((25, 4))\n",
        "pi_up[:, 0] = 1  # Assume up is the 0th action\n",
        "\n",
        "pi_down = np.zeros((25, 4))\n",
        "pi_down[:, 2] = 1  # Assume down is the 2nd action\n",
        "\n",
        "# Evaluate policies\n",
        "print(\"Policy Evaluation for Right:\")\n",
        "print(policy_evaluation(pi_right))\n",
        "print(\"Policy Evaluation for Left:\")\n",
        "print(policy_evaluation(pi_left))\n",
        "print(\"Policy Evaluation for Up:\")\n",
        "print(policy_evaluation(pi_up))\n",
        "print(\"Policy Evaluation for Down:\")\n",
        "print(policy_evaluation(pi_down))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0zmlgSY4UR"
      },
      "source": [
        "# Bonus : Implémentez policy itération et value itération (dans les slides que nous n'avons pas encore vu) et obtenez à l'aide de ces derniers pi* pour le Line World et le Grid World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-vUDJ_0ZJaO"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fcwX9miANL"
      },
      "source": [
        "# Naive Policy iteration implementation in order to find a Pi*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sluiZiiziKRk"
      },
      "outputs": [],
      "source": [
        "def naive_policy_iteration():\n",
        "  theta = 0.00001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  Pi = np.random.choice(A, len(S), True)\n",
        "\n",
        "  while True:\n",
        "    # Policy evaluation\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for s in range(len(S)):\n",
        "        v = V[s]\n",
        "        total = 0\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "        V[s] = total\n",
        "        delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      if s in T:\n",
        "        continue\n",
        "\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      # Compute Arg Max a\n",
        "      argmax_a = None\n",
        "      max_a = -999999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if argmax_a is None or total >= max_a:\n",
        "          argmax_a = a\n",
        "          max_a = total\n",
        "\n",
        "      Pi[s] = argmax_a\n",
        "\n",
        "      if old_action != Pi[s]:\n",
        "        policy_stable = False\n",
        "\n",
        "\n",
        "    if policy_stable:\n",
        "      break\n",
        "\n",
        "  return Pi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfiEE-u0jpql",
        "outputId": "806e8de8-49b6-4007-d7af-637fbd359214"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_policy_iteration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ONTRgCjr8q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
