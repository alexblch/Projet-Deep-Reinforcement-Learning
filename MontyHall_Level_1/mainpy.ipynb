{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monty Hall Environment\n",
    "\n",
    "L'agent est un candidat au jeu Monty Hall, Il doit prendre 2 décisions successives. Dans cet \n",
    "environnement, il y a 3 portes A, B et C. Au démarrage de l'environnement, une porte est tirée au hasard \n",
    "de manière cachée pour l'agent, il s'agit de la porte gagnante. La première action de l'agent est de choisir \n",
    "une porte parmi les trois portes. Ensuite, une porte parmi les 2 restantes non choisies par l'agent est \n",
    "retirée du jeu, il s'agit forcément d'une porte non gagnante. L'agent ensuite doit effectuer une nouvelle \n",
    "action : choisir de conserver la porte choisie au départ ou changer pour la porte restante. Une fois le choix \n",
    "fait, la porte choisie est 'ouverte' et l'on découvre si elle était gagnante (reward de 1.0) ou non (reward de \n",
    "0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe MontyHall Level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from montyhall import MontyHall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de partie manuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Naïf Q-learning\n",
    "\n",
    "L'algorithme naïf de Q-learning est une méthode d'apprentissage par renforcement sans modèle qui cherche à apprendre la politique optimale pour un agent en explorant et en exploitant un environnement. Cet algorithme utilise une table de valeurs Q pour stocker et mettre à jour les estimations des récompenses futures attendues pour chaque paire état-action. Voici les étapes clés de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La table Q est initialement remplie de valeurs arbitraires, et ces valeurs sont progressivement ajustées à travers les épisodes d'apprentissage.\n",
    "\n",
    "2. **Choix des actions** : À chaque étape, l'agent choisit une action basée soit sur l'exploration (choix aléatoire) pour découvrir de nouvelles stratégies, soit sur l'exploitation (choisir l'action avec la valeur Q la plus élevée pour l'état actuel) pour maximiser les récompenses.\n",
    "\n",
    "3. **Mise à jour de Q** : Après avoir pris une action, l'agent observe la récompense obtenue et le nouvel état atteint. La valeur Q pour la paire état-action est ensuite mise à jour en utilisant la formule :\n",
    "   \\[\n",
    "   Q(s, a) = Q(s, a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right]\n",
    "   \\]\n",
    "   où \\( \\alpha \\) est le taux d'apprentissage, \\( \\gamma \\) est le facteur de dépréciation, \\( r \\) est la récompense observée, \\( s' \\) est le nouvel état, et \\( a' \\) est la meilleure action possible dans le nouvel état.\n",
    "\n",
    "4. **Répétition** : Ces étapes sont répétées pour de nombreux épisodes jusqu'à ce que la table Q converge vers une approximation de la fonction de valeur optimale, permettant à l'agent de suivre la politique optimale déduite de Q.\n",
    "\n",
    "L'algorithme de Q-learning est particulièrement utile pour les problèmes avec un espace d'état et d'action discrets et peut être appliqué à une variété de tâches d'apprentissage et de décision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 11882.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 1}, {0: {0: 0.11547845827621962, 1: 0.3791201189325562}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_q_learning_gridworld(env_type, alpha: float = 0.1, epsilon: float = 0.1, gamma: float = 0.999, nb_iter: int = 100000):\n",
    "    Q = {}\n",
    "\n",
    "    env = env_type()\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if s not in Q:\n",
    "                Q[s] = {}\n",
    "                for a in aa:\n",
    "                    Q[s][a] = np.random.random()  # Initialize Q-values for each state-action pair\n",
    "\n",
    "            # Decide whether to explore or exploit\n",
    "            if np.random.random() < epsilon:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                # Choose the best action based on current Q-values\n",
    "                q_s = [Q[s][a] for a in aa]\n",
    "                best_a_index = np.argmax(q_s)\n",
    "                a = aa[best_a_index]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            s_p = env.state_id()\n",
    "            aa_p = env.available_actions()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                target = r\n",
    "            else:\n",
    "                if s_p not in Q:\n",
    "                    Q[s_p] = {}\n",
    "                    for a_p in aa_p:\n",
    "                        Q[s_p][a_p] = np.random.random()\n",
    "                q_s_p = [Q[s_p][a_p] for a_p in aa_p]\n",
    "                max_a_p = np.max(q_s_p)\n",
    "                target = r + gamma * max_a_p\n",
    "\n",
    "            Q[s][a] = (1 - alpha) * Q[s][a] + alpha * target\n",
    "\n",
    "    # Extract policy from Q-values\n",
    "    Pi = {}\n",
    "    for s, actions in Q.items():\n",
    "        best_a = max(actions, key=actions.get)\n",
    "        Pi[s] = best_a\n",
    "\n",
    "    return Pi, Q\n",
    "\n",
    "# Example usage:\n",
    "env = MontyHall\n",
    "naive_q_learning_gridworld(env_type=env, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Monte Carlo avec départs exploratoires (ES)\n",
    "\n",
    "L'algorithme de Monte Carlo avec départs exploratoires (ES) est une technique d'apprentissage par renforcement qui permet d'estimer la politique optimale en utilisant l'échantillonnage complet des retours (gains cumulés) de chaque épisode. Contrairement aux méthodes basées sur le temps différé comme Q-learning, les méthodes Monte Carlo ajustent les estimations de la politique uniquement à la fin de chaque épisode. Voici les étapes principales de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La politique (Pi) et la table de valeurs Q sont initialisées. Pour chaque état, une action est choisie aléatoirement comme action par défaut. Un dictionnaire de retours est également initialisé pour stocker les retours accumulés pour chaque paire état-action.\n",
    "\n",
    "2. **Départs exploratoires** : Chaque épisode commence dans un état initial choisi aléatoirement avec une action également choisie aléatoirement. Cela assure que toutes les paires état-action ont une chance d'être explorées suffisamment.\n",
    "\n",
    "3. **Génération de l'épisode** : L'agent suit la politique actuelle sauf pour le premier choix qui est aléatoire. L'épisode est enregistré sous forme de séquence de triplets (état, action, récompense).\n",
    "\n",
    "4. **Calcul du retour** : À la fin de l'épisode, le retour pour chaque étape est calculé en remontant depuis la fin de l'épisode jusqu'au début, en utilisant le facteur de dépréciation \\( \\gamma \\). Le retour est le gain cumulé à partir de cette étape jusqu'à la fin de l'épisode.\n",
    "\n",
    "5. **Mise à jour de Q et de la politique** :\n",
    "   - **Mise à jour de Q** : Pour chaque paire état-action unique rencontrée dans l'épisode, le retour est ajouté à la liste des retours correspondants et la valeur Q est mise à jour comme la moyenne de ces retours.\n",
    "   - **Mise à jour de la politique** : Pour chaque état visité, la politique est mise à jour pour choisir l'action qui maximise la valeur Q estimée pour cet état.\n",
    "\n",
    "6. **Répétition** : Le processus est répété pour un grand nombre d'épisodes pour permettre à la politique de converger vers la politique optimale.\n",
    "\n",
    "Cet algorithme est particulièrement utile dans les environnements avec des dynamiques simples où la méthode des départs exploratoires est faisable. Il est aussi bien adapté aux situations où il est possible de simuler l'environnement pour générer des épisodes avec des conditions de départ arbitraires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4487.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_monte_carlo_with_exploring_starts(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()  # Assurez-vous que cette méthode initialise correctement\n",
    "\n",
    "        is_first_action = True\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if s not in Pi:\n",
    "                Pi[s] = np.random.choice(aa)\n",
    "\n",
    "            if is_first_action:\n",
    "                a = np.random.choice(aa)\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        for (t, (s, a, r, aa)) in reversed(list(enumerate(trajectory))):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if all(map(lambda triplet: triplet[0] != s or triplet[1] != a, trajectory[:t])):\n",
    "                if (s, a) not in Returns:\n",
    "                    Returns[(s, a)] = []\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = None\n",
    "                best_a_score = 0.0\n",
    "                for a in aa:\n",
    "                    if (s, a) not in Q:\n",
    "                        Q[(s, a)] = np.random.random()\n",
    "                    if best_a is None or Q[(s, a)] > best_a_score:\n",
    "                        best_a = a\n",
    "                        best_a_score = Q[(s, a)]\n",
    "\n",
    "                Pi[s] = best_a\n",
    "    return Pi\n",
    "  \n",
    "naive_monte_carlo_with_exploring_starts(env, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme On-policy First visit Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3392.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def on_policy_first_visit_mc_control(env_type, gamma=0.999, epsilon=0.1, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    Returns = defaultdict(list)\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if np.random.random() < epsilon or s not in Pi:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        visited_state_action_pairs = set()\n",
    "        for (s, a, r, aa) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if (s, a) not in visited_state_action_pairs:\n",
    "                visited_state_action_pairs.add((s, a))\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = max(aa, key=lambda action: Q[(s, action)])\n",
    "                Pi[s] = best_a\n",
    "\n",
    "    return Pi\n",
    "\n",
    "on_policy_first_visit_mc_control(env, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off Policy First visit Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 17184.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def off_policy_mc_control(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    C = defaultdict(lambda: 0.0)\n",
    "    b = defaultdict(lambda: 1.0)  # behavior policy (random)\n",
    "\n",
    "    Pi = {}\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            a = np.random.choice(aa)\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for (s, a, r) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "            \n",
    "            Pi[s] = max(env.available_actions(), key=lambda action: Q[(s, action)])\n",
    "            \n",
    "            if a != Pi[s]:\n",
    "                break\n",
    "            \n",
    "            W *= 1.0 / b[(s, a)]\n",
    "\n",
    "    return Pi\n",
    "\n",
    "off_policy_mc_control(env, nb_iter=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Optimal Action 0\n",
      "State 1: Optimal Action 0\n",
      "State 2: Optimal Action 0\n",
      "State 3: Optimal Action 0\n",
      "State 4: Optimal Action 0\n",
      "State 5: Optimal Action 0\n",
      "State 6: Optimal Action 0\n",
      "State 7: Optimal Action 0\n",
      "State 8: Optimal Action 0\n",
      "State 9: Optimal Action 0\n",
      "State 10: Optimal Action 0\n",
      "State 11: Optimal Action 0\n",
      "State 0: Optimal Action 0\n",
      "State 1: Optimal Action 0\n",
      "State 2: Optimal Action 0\n",
      "State 3: Optimal Action 0\n",
      "State 4: Optimal Action 0\n",
      "State 5: Optimal Action 0\n",
      "State 6: Optimal Action 0\n",
      "State 7: Optimal Action 0\n",
      "State 8: Optimal Action 0\n",
      "State 9: Optimal Action 0\n",
      "State 10: Optimal Action 0\n",
      "State 11: Optimal Action 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sarsa(env_class, num_episodes, alpha, gamma, epsilon):\n",
    "    env = env_class()\n",
    "    num_states = env.num_states()\n",
    "    num_actions = env.num_actions()\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.state_id()\n",
    "        action = np.random.choice(env.available_actions()) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = np.random.choice(env.available_actions()) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
    "\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Parameters for SARSA\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes\n",
    "\n",
    "# Create the environment and run the SARSA algorithm\n",
    "env_class = MontyHall()\n",
    "Q = sarsa(env_class, num_episodes, alpha, gamma, epsilon)\n",
    "for state in range(env_class.num_states()):\n",
    "    print(f\"State {state}: Optimal Action {np.argmax(Q[state])}\")\n",
    "\n",
    "# Display the optimal policy found\n",
    "optimal_policy = np.argmax(Q, axis=1)\n",
    "for state in range(env_class().num_states()):\n",
    "    print(f\"State {state}: Optimal Action {optimal_policy[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: [0.0, 0.0]\n",
      "State 1: [0.0, 0.0]\n",
      "State 2: [0.0, 0.0]\n",
      "State 3: [0.0, 0.0]\n",
      "State 4: [0.0, 0.0]\n",
      "State 5: [0.0, 0.0]\n",
      "State 6: [0.0, 0.0]\n",
      "State 7: [0.0, 0.0]\n",
      "State 8: [0.0, 0.0]\n",
      "State 9: [0.0, 0.0]\n",
      "State 10: [0.0, 0.0]\n",
      "State 11: [0.0, 0.0]\n",
      "State 0: Stick\n",
      "State 1: Stick\n",
      "State 2: Stick\n",
      "State 3: Stick\n",
      "State 4: Stick\n",
      "State 5: Stick\n",
      "State 6: Stick\n",
      "State 7: Stick\n",
      "State 8: Stick\n",
      "State 9: Stick\n",
      "State 10: Stick\n",
      "State 11: Stick\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon, available_actions):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(available_actions)\n",
    "    else:\n",
    "        return max(available_actions, key=lambda x: Q[state][x])\n",
    "\n",
    "def dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps):\n",
    "    Q = {state: [0.0, 0.0] for state in range(env.num_states())}  # Only 2 actions: stick (0) or switch (1)\n",
    "    model = {state: {} for state in range(env.num_states())}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.state_id()\n",
    "        available_actions = env.available_actions()\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, available_actions)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_available_actions = env.available_actions()\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, next_available_actions)\n",
    "\n",
    "            # Update Q\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "            # Update the model\n",
    "            model[state][action] = (reward, next_state)\n",
    "\n",
    "            # Planning\n",
    "            for _ in range(planning_steps):\n",
    "                sampled_state = random.choice(list(model.keys()))\n",
    "                if not model[sampled_state]:\n",
    "                    continue\n",
    "                sampled_action = random.choice(list(model[sampled_state].keys()))\n",
    "                sampled_reward, sampled_next_state = model[sampled_state][sampled_action]\n",
    "\n",
    "                Q[sampled_state][sampled_action] += alpha * (sampled_reward + gamma * max(Q[sampled_next_state]) - Q[sampled_state][sampled_action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes\n",
    "planning_steps = 5  # Number of planning steps\n",
    "\n",
    "# Create the environment and run the Dyna-Q algorithm\n",
    "env = MontyHall()\n",
    "Q = dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps)\n",
    "for state, actions in Q.items():\n",
    "    print(f\"State {state}: {actions}\")\n",
    "\n",
    "# Display the optimal policy found\n",
    "for state in range(env.num_states()):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    action_symbols = ['Stick', 'Switch']\n",
    "    print(f\"State {state}: {action_symbols[best_action]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy et Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env_class, gamma=0.99, theta=1e-3):\n",
    "        self.env_class = env_class\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.env = env_class()\n",
    "        self.num_states = self.env.num_states()\n",
    "        self.num_actions = self.env.num_actions()\n",
    "\n",
    "    def run(self) -> Tuple[List[int], List[float]]:\n",
    "        policy = np.random.choice(self.num_actions, size=self.num_states)\n",
    "        V = np.zeros(self.num_states)\n",
    "\n",
    "        while True:\n",
    "            V = self.policy_evaluation(policy, V)\n",
    "            policy_stable = self.policy_improvement(policy, V)\n",
    "            if policy_stable:\n",
    "                break\n",
    "\n",
    "        return policy, V\n",
    "\n",
    "    def policy_evaluation(self, policy, V):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "                v = V[s]\n",
    "                new_v = 0\n",
    "                for a in range(self.num_actions):\n",
    "                    if policy[s] == a:\n",
    "                        for next_state in range(self.num_states):\n",
    "                            for r_index in range(self.env.num_rewards()):\n",
    "                                prob = self.env.p(s, a, next_state, r_index)\n",
    "                                reward = self.env.reward(r_index)\n",
    "                                new_v += prob * (reward + self.gamma * V[next_state])\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "                V[s] = new_v\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(self, policy, V):\n",
    "        policy_stable = True\n",
    "        for s in range(self.num_states):\n",
    "            chosen_a = policy[s]\n",
    "            action_values = self.one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = best_a\n",
    "        return policy_stable\n",
    "\n",
    "    def one_step_lookahead(self, state, V):\n",
    "        A = np.zeros(self.num_actions)\n",
    "        for a in range(self.num_actions):\n",
    "            for next_state in range(self.num_states):\n",
    "                for r_index in range(self.env.num_rewards()):\n",
    "                    prob = self.env.p(state, a, next_state, r_index)\n",
    "                    reward = self.env.reward(r_index)\n",
    "                    A[a] += prob * (reward + self.gamma * V[next_state])\n",
    "        return A\n",
    "\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env_class, gamma=0.99, theta=1e-3):\n",
    "        self.env_class = env_class\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.env = env_class()\n",
    "        self.num_states = self.env.num_states()\n",
    "        self.num_actions = self.env.num_actions()\n",
    "\n",
    "    def run(self) -> List[float]:\n",
    "        V = np.zeros(self.num_states)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "                v = V[s]\n",
    "                action_values = self.one_step_lookahead(s, V)\n",
    "                V[s] = max(action_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def one_step_lookahead(self, state, V):\n",
    "        A = np.zeros(self.num_actions)\n",
    "        for a in range(self.num_actions):\n",
    "            for next_state in range(self.num_states):\n",
    "                for r_index in range(self.env.num_rewards()):\n",
    "                    prob = self.env.p(state, a, next_state, r_index)\n",
    "                    reward = self.env.reward(r_index)\n",
    "                    A[a] += prob * (reward + self.gamma * V[next_state])\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Stick\n",
      "State 1: Stick\n",
      "State 2: Stick\n",
      "State 3: Stick\n",
      "State 4: Stick\n",
      "State 5: Stick\n",
      "State 6: Stick\n",
      "State 7: Stick\n",
      "State 8: Stick\n",
      "State 9: Stick\n",
      "State 10: Stick\n",
      "State 11: Stick\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_class = MontyHall\n",
    "\n",
    "# Run Policy Iteration\n",
    "pi = PolicyIteration(env_class)\n",
    "policy, V = pi.run()\n",
    "\n",
    "# Display the optimal policy found\n",
    "action_symbols = ['Stick', 'Switch']\n",
    "for state in range(env_class().num_states()):\n",
    "    print(f\"State {state}: {action_symbols[policy[state]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Value 1.0\n",
      "State 1: Value 0.0\n",
      "State 2: Value 0.0\n",
      "State 3: Value 0.0\n",
      "State 4: Value 0.0\n",
      "State 5: Value 0.0\n",
      "State 6: Value 0.0\n",
      "State 7: Value 0.0\n",
      "State 8: Value 0.0\n",
      "State 9: Value 0.0\n",
      "State 10: Value 0.0\n",
      "State 11: Value 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_class = MontyHall\n",
    "\n",
    "# Run Value Iteration\n",
    "vi = ValueIteration(env_class)\n",
    "V = vi.run()\n",
    "\n",
    "# Display the value function found\n",
    "for state in range(env_class().num_states()):\n",
    "    print(f\"State {state}: Value {V[state]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FrameworkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
