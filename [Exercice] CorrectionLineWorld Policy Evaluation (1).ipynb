{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "beryQv5c6FTP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ctypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19mv7NrhVZzx"
      },
      "source": [
        "#Définition d'un line world sous la forme d'un MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "psNAv57nOCMW"
      },
      "outputs": [],
      "source": [
        "S = [0, 1, 2 ,3, 4]\n",
        "A = [0, 1] # left right\n",
        "R = [-1, 0, 1]\n",
        "T = [0, 4]\n",
        "p = np.zeros((len(S), len(A), len(S), len(R))) # S, A, S_p, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tGlcGlTA6Tex"
      },
      "outputs": [],
      "source": [
        "for s in range(len(S)):\n",
        "  for a in range(len(A)):\n",
        "    for s_p in range(len(S)):\n",
        "      for r in range(len(R)):\n",
        "        if s_p == s + 1 and a == 1 and R[r] == 0 and s in [1, 2]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "        if s_p == s - 1 and a == 0 and R[r] == 0 and s in [2, 3]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "p[3, 1, 4, 2] = 1.0\n",
        "p[1, 0, 0, 0] = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnLQGzt4Vzxp"
      },
      "source": [
        "# Exercice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s43r9pUuWPtv"
      },
      "source": [
        "Implémentez une fonction prenant en paramètre une policy (un tableau numpy)\n",
        "et renvoyant la value de cette policy (un tableau numpy) pour chaque état du line world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_96znS5JWF9H"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy : np.ndarray, V: np.ndarray = None) -> np.ndarray:\n",
        "  theta = 0.00001\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  gamma = 0.999\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(len(S)):\n",
        "      v = V[s]\n",
        "      total = 0\n",
        "      for a in range(len(A)):\n",
        "        pi_s_a = policy[s, a]\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += pi_s_a * p[s, a, s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "      V[s] = total\n",
        "      delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "    if delta < theta:\n",
        "      return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29NHJWr3WTnW"
      },
      "source": [
        "Définissez une policy jouant tout le temps à droite.\n",
        "Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB8Gd0vPWiue",
        "outputId": "72ead850-6226-494a-816b-ed0ff637e9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.       0.998001 0.999    1.       0.      ]\n"
          ]
        }
      ],
      "source": [
        "pi_right = np.array([[0.0, 1.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_right))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIKxQ_fW1wC"
      },
      "source": [
        "Définissez une policy jouant tout le temps à gauche. Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-_ZCGmRW31Q",
        "outputId": "eddb5fdf-2017-4855-e711-d3071d1034dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.       -1.       -0.999    -0.998001  0.      ]\n"
          ]
        }
      ],
      "source": [
        "pi_left = np.array([[1.0, 0.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_left))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_8nQtoBXJai"
      },
      "source": [
        "Définissez une policy uniformément aléatoire (50% de chance d'aller à gauche et 50% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUYtT2hYW_Fe",
        "outputId": "55d1ad5b-ecff-478f-f031-4f4e78760b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00000000e+00 -4.99993115e-01  6.87814488e-06  5.00003436e-01\n",
            "  0.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "pi_random = np.array([[0.5, 0.5] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_random))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQpwh-x_XXGN"
      },
      "source": [
        "Définissez une policy uniformément aléatoire (15% de chance d'aller à gauche et 85% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-HkR7HCXc2b",
        "outputId": "dc51aa3e-5959-4b6d-9773-c9cac2875ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.6465129  0.93801492 0.99056154 0.        ]\n"
          ]
        }
      ],
      "source": [
        "pi_weird_random = np.array([[0.15, 0.85] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_weird_random))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nqSUVgCIis1z"
      },
      "outputs": [],
      "source": [
        "def naive_ineficient_policy_iteration():\n",
        "  theta = 0.000001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "\n",
        "  Pi = np.random.choice(A, len(S))\n",
        "\n",
        "  while True:\n",
        "    # Policy Evaluation\n",
        "    while True:\n",
        "      delta = 0.0\n",
        "\n",
        "      for s in S:\n",
        "        v = V[s]\n",
        "\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        V[s] = total\n",
        "\n",
        "        delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      best_a = None\n",
        "      best_action_score = -9999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if best_a is None or total >= best_action_score:\n",
        "          best_a = a\n",
        "          best_action_score = total\n",
        "\n",
        "      Pi[s] = best_a\n",
        "      if Pi[s] != old_action:\n",
        "        policy_stable = False\n",
        "\n",
        "    if policy_stable:\n",
        "      return Pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVMOCNl0j3Q-",
        "outputId": "ec4b0b57-c253-40a5-d823-0663e0efbf6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_ineficient_policy_iteration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZshaI9mXiaF"
      },
      "source": [
        "# Définissez une variante d'un GRID WORLD sous la forme d'un MDP et évaluez différentes stratégies sur cette environnement.\n",
        "\n",
        "Le grid world est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle l'agent peut évoluer, il commence généralement sur la première ligne, première colonne. L'agent possède 4 actions possibles (gauche, droite, haut, bas). Si jamais l'agent atteint la dernière case de la première ligne => état terminal avec reward de -3. Si jamais l'agent atteint la dernière case de la dernière ligne => état terminal avec reward de 1. Si l'agent essaye de se déplacer en dehors des bords de la grille => état terminal avec score de -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l_eC0-d9Xh7Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prochain état: (1, 0) Récompense: 0.0\n",
            "Prochain état: None Récompense: -1\n"
          ]
        }
      ],
      "source": [
        "S = np.zeros((5, 5)) \n",
        "\n",
        "\n",
        "A = [(0, -1), (0, 1), (-1, 0), (1,0)] # left right up down\n",
        "R = np.zeros((5, 5))\n",
        "R[0, 4] = -3\n",
        "R[4, 4] = 1\n",
        "\n",
        "\n",
        "p = np.zeros((S.shape[0]*S.shape[1], len(A), S.shape[0]*S.shape[1], R.shape[0] * R.shape[1])) # S, A, S_p, R\n",
        "\n",
        "# Fonction qui calcule l'état suivant et la récompense\n",
        "def transition(state, action):\n",
        "    new_state = (state[0] + action[0], state[1] + action[1])\n",
        "    if new_state[0] < 0 or new_state[0] >= 5 or new_state[1] < 0 or new_state[1] >= 5:\n",
        "        return None, -1  # Retourne None pour indiquer un état terminal, et une récompense de -1\n",
        "    else:\n",
        "        return new_state, R[new_state[0], new_state[1]]\n",
        "\n",
        "# Exemple d'utilisation de la fonction\n",
        "current_state = (0, 0)  \n",
        "action = (1, 0)         \n",
        "\n",
        "# Appliquer la transition\n",
        "next_state, reward = transition(current_state, action)\n",
        "print(\"Prochain état:\", next_state, \"Récompense:\", reward)\n",
        "\n",
        "# Tester un déplacement hors de la grille\n",
        "test_state = (0, 0)\n",
        "test_action = (-1, 0)  \n",
        "next_state, reward = transition(test_state, test_action)\n",
        "print(\"Prochain état:\", next_state, \"Récompense:\", reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation2(policy, grid_size, R, A,):\n",
        "    # Initialisation de la matrice des valeurs\n",
        "    V = np.zeros(grid_size)\n",
        "    discount_factor=0.999\n",
        "    theta=0.00001\n",
        "    # Boucle jusqu'à convergence\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(grid_size[0]):\n",
        "            for j in range(grid_size[1]):\n",
        "                v = V[i, j]\n",
        "                new_v = 0\n",
        "                action = A[policy[i, j]]  # Action déterminée par la politique\n",
        "                next_state, reward = transition((i, j), action)\n",
        "                if next_state is None:\n",
        "                    new_v = reward  # Si l'état suivant est terminal\n",
        "                else:\n",
        "                    new_v = reward + discount_factor * V[next_state[0], next_state[1]]\n",
        "                \n",
        "                V[i, j] = new_v\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        \n",
        "        # Condition de sortie de la boucle\n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.       -0.999    -0.998001 -0.997003 -0.996006]\n",
            " [-1.       -0.999    -0.998001 -0.997003 -0.996006]\n",
            " [-1.       -0.999    -0.998001 -0.997003 -0.996006]\n",
            " [-1.       -0.999    -0.998001 -0.997003 -0.996006]\n",
            " [-1.       -0.999    -0.998001 -0.997003 -0.996006]]\n",
            "[[-3.98701499e+00 -3.99100600e+00 -3.99500100e+00 -3.99900000e+00\n",
            "  -1.00000000e+00]\n",
            " [-9.96005996e-01 -9.97002999e-01 -9.98001000e-01 -9.99000000e-01\n",
            "  -1.00000000e+00]\n",
            " [-9.96005996e-01 -9.97002999e-01 -9.98001000e-01 -9.99000000e-01\n",
            "  -1.00000000e+00]\n",
            " [-9.96005996e-01 -9.97002999e-01 -9.98001000e-01 -9.99000000e-01\n",
            "  -1.00000000e+00]\n",
            " [ 9.97002999e-04  9.98001000e-04  9.99000000e-04  1.00000000e-03\n",
            "  -1.00000000e+00]]\n",
            "[[-1.         -1.         -1.         -1.         -1.        ]\n",
            " [-0.999      -0.999      -0.999      -0.999      -3.999     ]\n",
            " [-0.998001   -0.998001   -0.998001   -0.998001   -3.995001  ]\n",
            " [-0.997003   -0.997003   -0.997003   -0.997003   -3.991006  ]\n",
            " [-0.996006   -0.996006   -0.996006   -0.996006   -3.98701499]]\n",
            "[[-9.96005996e-01 -9.96005996e-01 -9.96005996e-01 -9.96005996e-01\n",
            "   9.97002999e-04]\n",
            " [-9.97002999e-01 -9.97002999e-01 -9.97002999e-01 -9.97002999e-01\n",
            "   9.98001000e-04]\n",
            " [-9.98001000e-01 -9.98001000e-01 -9.98001000e-01 -9.98001000e-01\n",
            "   9.99000000e-04]\n",
            " [-9.99000000e-01 -9.99000000e-01 -9.99000000e-01 -9.99000000e-01\n",
            "   1.00000000e-03]\n",
            " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
            "  -1.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "grid_size = (5, 5)\n",
        "pi_left = np.zeros((5, 5), dtype=int)\n",
        "print(policy_evaluation2(pi_left, S.shape, R, A))\n",
        "\n",
        "pi_right = np.ones(grid_size, dtype=int) * 1  # Fill the matrix with 1s\n",
        "print(policy_evaluation2(pi_right, S.shape, R, A))\n",
        "# Policy that always moves up\n",
        "pi_up = np.ones(grid_size, dtype=int) * 2     # Fill the matrix with 2s\n",
        "print(policy_evaluation2(pi_up, S.shape, R, A))\n",
        "# Policy that always moves down\n",
        "pi_down = np.ones(grid_size, dtype=int) * 3   # Fill the matrix with 3s\n",
        "print(policy_evaluation2(pi_down, S.shape, R, A))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZTA1y6YwkJ"
      },
      "source": [
        "# Proposez plusieurs stratégies et évaluez les à l'aide de policy évaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ve8dvoONYuq5"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for axis 1 with size 2",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pi_right \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(S))])\n\u001b[1;32m----> 2\u001b[0m policy_evaluation(pi_right)\n",
            "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[1;34m(policy, V)\u001b[0m\n\u001b[0;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(A)):\n\u001b[1;32m---> 13\u001b[0m   pi_s_a \u001b[38;5;241m=\u001b[39m policy[s, a]\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m s_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(S)):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(R)):\n",
            "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
          ]
        }
      ],
      "source": [
        "pi_right = np.array([[0.0, 1.0] for _ in range(len(S))])\n",
        "policy_evaluation(pi_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0zmlgSY4UR"
      },
      "source": [
        "# Bonus : Implémentez policy itération et value itération (dans les slides que nous n'avons pas encore vu) et obtenez à l'aide de ces derniers pi* pour le Line World et le Grid World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J-vUDJ_0ZJaO"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fcwX9miANL"
      },
      "source": [
        "# Naive Policy iteration implementation in order to find a Pi*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sluiZiiziKRk"
      },
      "outputs": [],
      "source": [
        "def naive_policy_iteration():\n",
        "  theta = 0.00001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  Pi = np.random.choice(A, len(S), True)\n",
        "\n",
        "  while True:\n",
        "    # Policy evaluation\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for s in range(len(S)):\n",
        "        v = V[s]\n",
        "        total = 0\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "        V[s] = total\n",
        "        delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      if s in T:\n",
        "        continue\n",
        "\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      # Compute Arg Max a\n",
        "      argmax_a = None\n",
        "      max_a = -999999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if argmax_a is None or total >= max_a:\n",
        "          argmax_a = a\n",
        "          max_a = total\n",
        "\n",
        "      Pi[s] = argmax_a\n",
        "\n",
        "      if old_action != Pi[s]:\n",
        "        policy_stable = False\n",
        "\n",
        "\n",
        "    if policy_stable:\n",
        "      break\n",
        "\n",
        "  return Pi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfiEE-u0jpql",
        "outputId": "806e8de8-49b6-4007-d7af-637fbd359214"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "a must be 1-dimensional",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m naive_policy_iteration()\n",
            "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mnaive_policy_iteration\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m T:\n\u001b[0;32m      7\u001b[0m   V[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m Pi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(A, \u001b[38;5;28mlen\u001b[39m(S), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# Policy evaluation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "File \u001b[1;32mmtrand.pyx:911\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: a must be 1-dimensional"
          ]
        }
      ],
      "source": [
        "naive_policy_iteration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ONTRgCjr8q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
