{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GridWorld** est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle un agent peut se déplacer. L'agent commence généralement dans la première case de la grille (première ligne, première colonne). Il a quatre actions possibles à sa disposition :\n",
    "- **Gauche** : se déplacer vers la gauche.\n",
    "- **Droite** : se déplacer vers la droite.\n",
    "- **Haut** : se déplacer vers le haut.\n",
    "- **Bas** : se déplacer vers le bas.\n",
    "\n",
    "**Conditions terminales et récompenses :**\n",
    "- Atteindre la dernière case de la première ligne (position [0, 4]) est un état terminal et donne une récompense de -3.\n",
    "- Atteindre la dernière case de la dernière ligne (position [4, 4]) est également un état terminal, mais avec une récompense de 1.\n",
    "- Essayer de se déplacer en dehors des bords de la grille entraîne un état terminal avec une récompense de -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de partie manuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ X _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "grid.step(1)\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ X _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "grid.step(3)\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ X _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "grid.step(3)\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ X _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "grid.step(1)\n",
    "grid.step(1)\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ X _ \n"
     ]
    }
   ],
   "source": [
    "grid.step(3)\n",
    "grid.step(3)\n",
    "grid.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ _ _ _ X \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.step(1)\n",
    "grid.display()\n",
    "grid.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Naïf Q-learning\n",
    "\n",
    "L'algorithme naïf de Q-learning est une méthode d'apprentissage par renforcement sans modèle qui cherche à apprendre la politique optimale pour un agent en explorant et en exploitant un environnement. Cet algorithme utilise une table de valeurs Q pour stocker et mettre à jour les estimations des récompenses futures attendues pour chaque paire état-action. Voici les étapes clés de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La table Q est initialement remplie de valeurs arbitraires, et ces valeurs sont progressivement ajustées à travers les épisodes d'apprentissage.\n",
    "\n",
    "2. **Choix des actions** : À chaque étape, l'agent choisit une action basée soit sur l'exploration (choix aléatoire) pour découvrir de nouvelles stratégies, soit sur l'exploitation (choisir l'action avec la valeur Q la plus élevée pour l'état actuel) pour maximiser les récompenses.\n",
    "\n",
    "3. **Mise à jour de Q** : Après avoir pris une action, l'agent observe la récompense obtenue et le nouvel état atteint. La valeur Q pour la paire état-action est ensuite mise à jour en utilisant la formule :\n",
    "   \\[\n",
    "   Q(s, a) = Q(s, a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right]\n",
    "   \\]\n",
    "   où \\( \\alpha \\) est le taux d'apprentissage, \\( \\gamma \\) est le facteur de dépréciation, \\( r \\) est la récompense observée, \\( s' \\) est le nouvel état, et \\( a' \\) est la meilleure action possible dans le nouvel état.\n",
    "\n",
    "4. **Répétition** : Ces étapes sont répétées pour de nombreux épisodes jusqu'à ce que la table Q converge vers une approximation de la fonction de valeur optimale, permettant à l'agent de suivre la politique optimale déduite de Q.\n",
    "\n",
    "L'algorithme de Q-learning est particulièrement utile pour les problèmes avec un espace d'état et d'action discrets et peut être appliqué à une variété de tâches d'apprentissage et de décision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3625.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 0): 1,\n",
       "  (0, 1): 1,\n",
       "  (1, 1): 1,\n",
       "  (1, 0): 1,\n",
       "  (2, 0): 3,\n",
       "  (3, 0): 3,\n",
       "  (0, 2): 1,\n",
       "  (1, 2): 2,\n",
       "  (1, 3): 2,\n",
       "  (2, 2): 2,\n",
       "  (2, 3): 2,\n",
       "  (2, 1): 1,\n",
       "  (3, 1): 3,\n",
       "  (3, 2): 1,\n",
       "  (3, 3): 2,\n",
       "  (4, 2): 3,\n",
       "  (4, 1): 0,\n",
       "  (0, 3): 2,\n",
       "  (2, 4): 0,\n",
       "  (1, 4): 2,\n",
       "  (0, 4): 1,\n",
       "  (3, 4): 0,\n",
       "  (4, 3): 3},\n",
       " {(0, 0): {1: 0.9930209650349772, 3: 0.9930209650349772},\n",
       "  (0, 1): {1: 0.994014980014992, 2: 0.9818677420978968, 3: 0.9176136655553827},\n",
       "  (1, 1): {0: 0.993020965034559,\n",
       "   1: 0.9950099900049977,\n",
       "   2: 0.993020965034758,\n",
       "   3: 0.9930209650341054},\n",
       "  (1, 0): {0: 0.9920279440699412, 1: 0.994014980014992, 3: 0.994014980014992},\n",
       "  (2, 0): {0: 0.9757763486007596,\n",
       "   1: 0.9786948630077604,\n",
       "   3: 0.9950099900049977},\n",
       "  (3, 0): {0: 0.9183594647516775, 1: -2.98820091436854, 3: 0.9960059960009989},\n",
       "  (0, 2): {1: 0.9734175304072404, 2: 0.7981644727735243, 3: 0.752418872742548},\n",
       "  (1, 2): {0: 0.7839501082225314,\n",
       "   1: 0.8651642261539003,\n",
       "   2: 0.994014980014992,\n",
       "   3: 0.8430096037557249},\n",
       "  (1, 3): {0: 0.7525063483755321,\n",
       "   1: 0.7525365495509735,\n",
       "   2: 0.9637286177355466,\n",
       "   3: 0.7618416960672617},\n",
       "  (2, 2): {0: 0.938023974372516,\n",
       "   1: 0.9760727307596532,\n",
       "   2: 0.9950099900049977,\n",
       "   3: 0.9295690932039662},\n",
       "  (2, 3): {0: 0.7525030230966616,\n",
       "   1: 0.6960180257849222,\n",
       "   2: 0.9893167800482519,\n",
       "   3: 0.7140423233405232},\n",
       "  (2, 1): {0: 0.9940149800149884,\n",
       "   1: 0.9960059960009989,\n",
       "   2: 0.9940149800149911,\n",
       "   3: 0.9940149800148741},\n",
       "  (3, 1): {0: 0.995009990004992,\n",
       "   1: 0.9950099900049952,\n",
       "   2: 0.9950099900049646,\n",
       "   3: 0.9970029989999992},\n",
       "  (3, 2): {0: 0.9940149800149722,\n",
       "   1: 0.9980009999999991,\n",
       "   2: 0.9960059960009929,\n",
       "   3: 0.9960059960001422},\n",
       "  (3, 3): {0: 0.8969615978741297,\n",
       "   1: 0.9518363313178263,\n",
       "   2: 0.9970029989999992,\n",
       "   3: 0.6441639912138596},\n",
       "  (4, 2): {0: 0.9970029989999992,\n",
       "   2: 0.9950099900049977,\n",
       "   3: 0.9989999999999996},\n",
       "  (4, 1): {0: 0.9960059960009989,\n",
       "   2: -2.9830347170122824,\n",
       "   3: 0.9791532946970191},\n",
       "  (0, 3): {1: 0.7525167758305976,\n",
       "   2: 0.7742595037091085,\n",
       "   3: 0.7448131745752384},\n",
       "  (2, 4): {0: 0.7925430941625197,\n",
       "   1: 0.7280896809986376,\n",
       "   2: 0.7489018898532922},\n",
       "  (1, 4): {0: 0.752447149656351, 1: 0.7542597925024352, 2: 0.8668007037320012},\n",
       "  (0, 4): {1: 0.752439892612778, 2: 0.7174080615166779},\n",
       "  (3, 4): {0: 0.7632227339583177,\n",
       "   1: 0.7045671719057496,\n",
       "   2: 0.0025248763427160315},\n",
       "  (4, 3): {0: 0.9960059960009982,\n",
       "   2: 0.9980009999999991,\n",
       "   3: 0.9999999999999994}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_q_learning_gridworld(env_type, alpha: float = 0.1, epsilon: float = 0.1, gamma: float = 0.999, nb_iter: int = 100000):\n",
    "    Q = {}\n",
    "\n",
    "    env = env_type()\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if s not in Q:\n",
    "                Q[s] = {}\n",
    "                for a in aa:\n",
    "                    Q[s][a] = np.random.random()  # Initialize Q-values for each state-action pair\n",
    "\n",
    "            # Decide whether to explore or exploit\n",
    "            if np.random.random() < epsilon:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                # Choose the best action based on current Q-values\n",
    "                q_s = [Q[s][a] for a in aa]\n",
    "                best_a_index = np.argmax(q_s)\n",
    "                a = aa[best_a_index]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            s_p = env.state_id()\n",
    "            aa_p = env.available_actions()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                target = r\n",
    "            else:\n",
    "                if s_p not in Q:\n",
    "                    Q[s_p] = {}\n",
    "                    for a_p in aa_p:\n",
    "                        Q[s_p][a_p] = np.random.random()\n",
    "                q_s_p = [Q[s_p][a_p] for a_p in aa_p]\n",
    "                max_a_p = np.max(q_s_p)\n",
    "                target = r + gamma * max_a_p\n",
    "\n",
    "            Q[s][a] = (1 - alpha) * Q[s][a] + alpha * target\n",
    "\n",
    "    # Extract policy from Q-values\n",
    "    Pi = {}\n",
    "    for s, actions in Q.items():\n",
    "        best_a = max(actions, key=actions.get)\n",
    "        Pi[s] = best_a\n",
    "\n",
    "    return Pi, Q\n",
    "\n",
    "# Example usage:\n",
    "naive_q_learning_gridworld(GridWorld, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme Monte Carlo avec départs exploratoires (ES)\n",
    "\n",
    "L'algorithme de Monte Carlo avec départs exploratoires (ES) est une technique d'apprentissage par renforcement qui permet d'estimer la politique optimale en utilisant l'échantillonnage complet des retours (gains cumulés) de chaque épisode. Contrairement aux méthodes basées sur le temps différé comme Q-learning, les méthodes Monte Carlo ajustent les estimations de la politique uniquement à la fin de chaque épisode. Voici les étapes principales de cet algorithme :\n",
    "\n",
    "1. **Initialisation** : La politique (Pi) et la table de valeurs Q sont initialisées. Pour chaque état, une action est choisie aléatoirement comme action par défaut. Un dictionnaire de retours est également initialisé pour stocker les retours accumulés pour chaque paire état-action.\n",
    "\n",
    "2. **Départs exploratoires** : Chaque épisode commence dans un état initial choisi aléatoirement avec une action également choisie aléatoirement. Cela assure que toutes les paires état-action ont une chance d'être explorées suffisamment.\n",
    "\n",
    "3. **Génération de l'épisode** : L'agent suit la politique actuelle sauf pour le premier choix qui est aléatoire. L'épisode est enregistré sous forme de séquence de triplets (état, action, récompense).\n",
    "\n",
    "4. **Calcul du retour** : À la fin de l'épisode, le retour pour chaque étape est calculé en remontant depuis la fin de l'épisode jusqu'au début, en utilisant le facteur de dépréciation \\( \\gamma \\). Le retour est le gain cumulé à partir de cette étape jusqu'à la fin de l'épisode.\n",
    "\n",
    "5. **Mise à jour de Q et de la politique** :\n",
    "   - **Mise à jour de Q** : Pour chaque paire état-action unique rencontrée dans l'épisode, le retour est ajouté à la liste des retours correspondants et la valeur Q est mise à jour comme la moyenne de ces retours.\n",
    "   - **Mise à jour de la politique** : Pour chaque état visité, la politique est mise à jour pour choisir l'action qui maximise la valeur Q estimée pour cet état.\n",
    "\n",
    "6. **Répétition** : Le processus est répété pour un grand nombre d'épisodes pour permettre à la politique de converger vers la politique optimale.\n",
    "\n",
    "Cet algorithme est particulièrement utile dans les environnements avec des dynamiques simples où la méthode des départs exploratoires est faisable. Il est aussi bien adapté aux situations où il est possible de simuler l'environnement pour générer des épisodes avec des conditions de départ arbitraires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2057.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, 3): 1,\n",
       " (0, 3): 1,\n",
       " (0, 4): 1,\n",
       " (0, 2): 1,\n",
       " (0, 1): 3,\n",
       " (1, 1): 1,\n",
       " (1, 2): 1,\n",
       " (3, 1): 3,\n",
       " (4, 1): 0,\n",
       " (2, 3): 1,\n",
       " (2, 2): 3,\n",
       " (3, 3): 3,\n",
       " (4, 3): 3,\n",
       " (4, 2): 3,\n",
       " (3, 4): 1,\n",
       " (3, 2): 1,\n",
       " (2, 1): 1,\n",
       " (1, 0): 1,\n",
       " (2, 0): 3,\n",
       " (3, 0): 3,\n",
       " (0, 0): 3,\n",
       " (1, 4): 2,\n",
       " (2, 4): 1}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_monte_carlo_with_exploring_starts(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()  # Assurez-vous que cette méthode initialise correctement\n",
    "\n",
    "        is_first_action = True\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if s not in Pi:\n",
    "                Pi[s] = np.random.choice(aa)\n",
    "\n",
    "            if is_first_action:\n",
    "                a = np.random.choice(aa)\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        for (t, (s, a, r, aa)) in reversed(list(enumerate(trajectory))):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if all(map(lambda triplet: triplet[0] != s or triplet[1] != a, trajectory[:t])):\n",
    "                if (s, a) not in Returns:\n",
    "                    Returns[(s, a)] = []\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = None\n",
    "                best_a_score = 0.0\n",
    "                for a in aa:\n",
    "                    if (s, a) not in Q:\n",
    "                        Q[(s, a)] = np.random.random()\n",
    "                    if best_a is None or Q[(s, a)] > best_a_score:\n",
    "                        best_a = a\n",
    "                        best_a_score = Q[(s, a)]\n",
    "\n",
    "                Pi[s] = best_a\n",
    "    return Pi\n",
    "  \n",
    "naive_monte_carlo_with_exploring_starts(GridWorld, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme On-policy First visit Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1770.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 1): 3,\n",
       " (0, 0): 1,\n",
       " (1, 1): 1,\n",
       " (1, 2): 3,\n",
       " (1, 0): 3,\n",
       " (2, 1): 3,\n",
       " (2, 2): 3,\n",
       " (3, 2): 3,\n",
       " (3, 1): 3,\n",
       " (3, 0): 3,\n",
       " (1, 4): 1,\n",
       " (0, 4): 1,\n",
       " (2, 4): 1,\n",
       " (2, 3): 3,\n",
       " (1, 3): 1,\n",
       " (0, 3): 3,\n",
       " (0, 2): 3,\n",
       " (4, 1): 0,\n",
       " (3, 3): 3,\n",
       " (4, 3): 3,\n",
       " (2, 0): 3,\n",
       " (4, 2): 0,\n",
       " (3, 4): 1}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def on_policy_first_visit_mc_control(env_type, gamma=0.999, epsilon=0.1, nb_iter=10000, max_steps=10):\n",
    "    Pi = {}\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    Returns = defaultdict(list)\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            if np.random.random() < epsilon or s not in Pi:\n",
    "                a = np.random.choice(aa)\n",
    "            else:\n",
    "                a = Pi[s]\n",
    "\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r, aa))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        visited_state_action_pairs = set()\n",
    "        for (s, a, r, aa) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if (s, a) not in visited_state_action_pairs:\n",
    "                visited_state_action_pairs.add((s, a))\n",
    "                Returns[(s, a)].append(G)\n",
    "                Q[(s, a)] = np.mean(Returns[(s, a)])\n",
    "\n",
    "                best_a = max(aa, key=lambda action: Q[(s, action)])\n",
    "                Pi[s] = best_a\n",
    "\n",
    "    return Pi\n",
    "\n",
    "on_policy_first_visit_mc_control(GridWorld, nb_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off Policy First visit Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7674.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, 4): 0,\n",
       " (3, 0): 0,\n",
       " (0, 3): 1,\n",
       " (3, 1): 0,\n",
       " (0, 1): 1,\n",
       " (2, 3): 0,\n",
       " (2, 4): 0,\n",
       " (0, 2): 1,\n",
       " (4, 2): 0,\n",
       " (3, 2): 0,\n",
       " (0, 4): 1,\n",
       " (3, 4): 0,\n",
       " (3, 3): 0,\n",
       " (1, 2): 0,\n",
       " (4, 1): 0,\n",
       " (4, 3): 0,\n",
       " (2, 1): 0,\n",
       " (1, 3): 0,\n",
       " (2, 0): 0,\n",
       " (2, 2): 0,\n",
       " (1, 0): 0,\n",
       " (1, 1): 0,\n",
       " (0, 0): 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def off_policy_mc_control(env_type, gamma=0.999, nb_iter=10000, max_steps=10):\n",
    "    Q = defaultdict(lambda: 0.0)\n",
    "    C = defaultdict(lambda: 0.0)\n",
    "    b = defaultdict(lambda: 1.0)  # behavior policy (random)\n",
    "\n",
    "    Pi = {}\n",
    "\n",
    "    for it in tqdm(range(nb_iter)):\n",
    "        env = env_type.from_random_state()\n",
    "        trajectory = []\n",
    "        steps_count = 0\n",
    "\n",
    "        while not env.is_game_over() and steps_count < max_steps:\n",
    "            s = env.state_id()\n",
    "            aa = env.available_actions()\n",
    "\n",
    "            a = np.random.choice(aa)\n",
    "            prev_score = env.score()\n",
    "            env.step(a)\n",
    "            r = env.score() - prev_score\n",
    "\n",
    "            trajectory.append((s, a, r))\n",
    "            steps_count += 1\n",
    "\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for (s, a, r) in reversed(trajectory):\n",
    "            G = gamma * G + r\n",
    "            C[(s, a)] += W\n",
    "            Q[(s, a)] += (W / C[(s, a)]) * (G - Q[(s, a)])\n",
    "            \n",
    "            Pi[s] = max(env.available_actions(), key=lambda action: Q[(s, action)])\n",
    "            \n",
    "            if a != Pi[s]:\n",
    "                break\n",
    "            \n",
    "            W *= 1.0 / b[(s, a)]\n",
    "\n",
    "    return Pi\n",
    "\n",
    "off_policy_mc_control(GridWorld, nb_iter=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon, available_actions):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(available_actions)\n",
    "    else:\n",
    "        return max(available_actions, key=lambda x: Q[state][x])\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma, epsilon):\n",
    "    Q = {}\n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            Q[(col, row)] = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.state_id()\n",
    "        available_actions = env.available_actions()\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, available_actions)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            env.step(action)\n",
    "            reward = env.score()\n",
    "            next_state = env.state_id()\n",
    "            next_available_actions = env.available_actions()\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, next_available_actions)\n",
    "\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ ↓ ← ← ← \n",
      "→ ↓ ← ← ← \n",
      "→ ↓ ← ← ← \n",
      "→ ↓ ← ← ← \n",
      "→ → → → ← \n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.99  # Facteur de réduction\n",
    "epsilon = 0.1  # Taux d'exploration\n",
    "num_episodes = 1000  # Nombre d'épisodes\n",
    "\n",
    "# Création de l'environnement et exécution de l'algorithme SARSA\n",
    "env = GridWorld()\n",
    "Q = sarsa(env, num_episodes, alpha, gamma, epsilon)\n",
    "\n",
    "# Affichage de la politique optimale trouvée\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        state = (col, row)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        action_symbols = ['←', '→', '↑', '↓']\n",
    "        print(action_symbols[best_action], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ ↓ ↓ ← ← \n",
      "↓ ↓ ↓ ← ← \n",
      "→ → ↓ ↓ ← \n",
      "→ → → ↓ ← \n",
      "→ → → → ← \n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon, available_actions):\n",
    "    if not available_actions:\n",
    "        return random.choice([0, 1, 2, 3])\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(available_actions)\n",
    "    else:\n",
    "        if available_actions:\n",
    "            return max(available_actions, key=lambda x: Q[state][x])\n",
    "        else:\n",
    "            return random.choice([0, 1, 2, 3])\n",
    "\n",
    "def dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps):\n",
    "    Q = {}\n",
    "    model = {}\n",
    "\n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            Q[(col, row)] = [0.0, 0.0, 0.0, 0.0]\n",
    "            model[(col, row)] = {}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.state_id()\n",
    "        available_actions = env.available_actions()\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, available_actions)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            env.step(action)\n",
    "            reward = env.score()\n",
    "            next_state = env.state_id()\n",
    "            next_available_actions = env.available_actions()\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, next_available_actions)\n",
    "\n",
    "            # Mise à jour de Q\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "            # Mise à jour du modèle\n",
    "            if action not in model[state]:\n",
    "                model[state][action] = (reward, next_state)\n",
    "            else:\n",
    "                model[state][action] = (reward, next_state)\n",
    "\n",
    "            # Planification\n",
    "            for _ in range(planning_steps):\n",
    "                sampled_state = random.choice(list(model.keys()))\n",
    "                if not model[sampled_state]:\n",
    "                    continue\n",
    "                sampled_action = random.choice(list(model[sampled_state].keys()))\n",
    "                sampled_reward, sampled_next_state = model[sampled_state][sampled_action]\n",
    "\n",
    "                Q[sampled_state][sampled_action] += alpha * (sampled_reward + gamma * max(Q[sampled_next_state]) - Q[sampled_state][sampled_action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Paramètres\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.99  # Facteur de réduction\n",
    "epsilon = 0.1  # Taux d'exploration\n",
    "num_episodes = 1000  # Nombre d'épisodes\n",
    "planning_steps = 5  # Nombre d'étapes de planification\n",
    "\n",
    "# Création de l'environnement et exécution de l'algorithme Dyna-Q\n",
    "env = GridWorld()\n",
    "Q = dyna_q(env, num_episodes, alpha, gamma, epsilon, planning_steps)\n",
    "\n",
    "# Affichage de la politique optimale trouvée\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        state = (col, row)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        action_symbols = ['←', '→', '↑', '↓']\n",
    "        print(action_symbols[best_action], end=' ')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FrameworkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
