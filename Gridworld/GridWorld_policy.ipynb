{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld et policy_iteration\n",
    "Le **policy iteration** est un algorithme de programmation dynamique utilisé pour résoudre des problèmes de décision markovienne (MDP). Il s'agit de trouver une politique qui maximise la somme des récompenses attendues sur le long terme pour chaque état du processus. L'algorithme de policy iteration comprend deux phases principales : évaluation de la politique et amélioration de la politique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.S = (size, size)  # grid size\n",
    "        self.A = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # actions: right, left, down, up\n",
    "        self.terminal = [(0, 4), (4, 4)]  # terminal states\n",
    "        self.R = {(0, 4): -3, (4, 4): 1}  # rewards for terminal states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state in self.terminal\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if 0 <= next_state[0] < self.S[0] and 0 <= next_state[1] < self.S[1]:\n",
    "            return next_state\n",
    "        return state  # return the current state if the action leads out of bounds\n",
    "    \n",
    "    def reset(self, size=5):\n",
    "        self.S = (size, size)  # grid size\n",
    "        self.A = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # actions: right, left, down, up\n",
    "        self.terminal = [(0, 4), (4, 4)]  # terminal states\n",
    "        self.R = {(0, 4): -3, (4, 4): 1}  # rewards for terminal states\n",
    "        \n",
    "        \n",
    "def policy_iteration(env, gamma=0.9, theta=0.0001):\n",
    "    policy = np.zeros((env.S[0], env.S[1], len(env.A))) + 1 / len(env.A)  # Start with a uniform random policy\n",
    "    stable_policy = False  # Flag to check if the policy has stabilized\n",
    "\n",
    "    while not stable_policy:\n",
    "        # Policy Evaluation\n",
    "        V = np.zeros(env.S)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.S[0] * env.S[1]):\n",
    "                x, y = s // env.S[1], s % env.S[1]\n",
    "                if env.is_terminal((x, y)):\n",
    "                    continue\n",
    "                v = V[x, y]\n",
    "                new_v = 0\n",
    "                for a, action in enumerate(env.A):\n",
    "                    next_state = env.next_state((x, y), action)\n",
    "                    reward = env.R.get(next_state, 0)\n",
    "                    new_v += policy[x, y, a] * (reward + gamma * V[next_state[0], next_state[1]])\n",
    "                V[x, y] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        stable_policy = True\n",
    "        for s in range(env.S[0] * env.S[1]):\n",
    "            x, y = s // env.S[1], s % env.S[1]\n",
    "            if env.is_terminal((x, y)):\n",
    "                continue\n",
    "\n",
    "            old_action = np.argmax(policy[x, y])\n",
    "            action_values = np.zeros(len(env.A))\n",
    "            for a, action in enumerate(env.A):\n",
    "                next_state = env.next_state((x, y), action)\n",
    "                reward = env.R.get(next_state, 0)\n",
    "                action_values[a] = reward + gamma * V[next_state[0], next_state[1]]\n",
    "\n",
    "            best_action = np.argmax(action_values)\n",
    "            if old_action != best_action:\n",
    "                stable_policy = False\n",
    "            policy[x, y] = np.eye(len(env.A))[best_action]  # Make the policy greedy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "[[[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   0.   1.   0.  ]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   0.   1.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   0.   1.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.   0.   1.   0.  ]]\n",
      "\n",
      " [[1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [1.   0.   0.   0.  ]\n",
      "  [0.25 0.25 0.25 0.25]]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "#all directions\n",
    "policy, V = policy_iteration(env)\n",
    "print('Policy:')\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.4782969 0.531441  0.59049   0.6561    0.       ]\n",
      " [0.531441  0.59049   0.6561    0.729     0.81     ]\n",
      " [0.59049   0.6561    0.729     0.81      0.9      ]\n",
      " [0.6561    0.729     0.81      0.9       1.       ]\n",
      " [0.729     0.81      0.9       1.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "print('Value function:')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration dans GridWorld\n",
    "\n",
    "La Value Iteration est une méthode de programmation dynamique pour trouver la politique optimale dans un processus de décision markovien (MDP). Cette approche est particulièrement efficace pour les environnements avec un espace d'états et d'actions finis, comme dans le cas d'un GridWorld.\n",
    "\n",
    "### Principe de l'algorithme :\n",
    "\n",
    "1. **Initialisation** :\n",
    "   - Initialisez la fonction de valeur V pour tous les états à zéro.\n",
    "\n",
    "2. **Iteration** :\n",
    "   - Pour chaque état, calculez la valeur maximale en prenant en compte toutes les actions possibles depuis cet état. Mettez à jour la valeur de l'état avec cette valeur maximale.\n",
    "   - Répétez ce processus pour tous les états.\n",
    "   - Continuez cette étape d'itération jusqu'à ce que la différence maximale entre les anciennes valeurs et les nouvelles valeurs des états (delta) soit inférieure à un petit seuil (theta), indiquant la convergence de l'algorithme.\n",
    "\n",
    "3. **Extraction de la politique** :\n",
    "   - Une fois la fonction de valeur optimale obtenue, dérivez la politique optimale. Pour chaque état, choisissez l'action qui maximise la somme de la récompense immédiate et la valeur escomptée des états futurs selon la fonction de valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]]),\n",
       " array([[0.4782969, 0.531441 , 0.59049  , 0.6561   , 0.       ],\n",
       "        [0.531441 , 0.59049  , 0.6561   , 0.729    , 0.81     ],\n",
       "        [0.59049  , 0.6561   , 0.729    , 0.81     , 0.9      ],\n",
       "        [0.6561   , 0.729    , 0.81     , 0.9      , 1.       ],\n",
       "        [0.729    , 0.81     , 0.9      , 1.       , 0.       ]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=0.0001):\n",
    "    V = np.zeros(env.S)  # Initialize V to zero for all states\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.S[0] * env.S[1]):\n",
    "            x, y = s // env.S[1], s % env.S[1]\n",
    "            if env.is_terminal((x, y)):\n",
    "                continue\n",
    "            v = V[x, y]\n",
    "            max_value = float('-inf')\n",
    "            for action in env.A:\n",
    "                next_state = env.next_state((x, y), action)\n",
    "                reward = env.R.get(next_state, 0)\n",
    "                value = reward + gamma * V[next_state[0], next_state[1]]\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "            V[x, y] = max_value\n",
    "            delta = max(delta, abs(v - max_value))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Derive policy from value function\n",
    "    policy = np.zeros((env.S[0], env.S[1], len(env.A)))\n",
    "    for s in range(env.S[0] * env.S[1]):\n",
    "        x, y = s // env.S[1], s % env.S[1]\n",
    "        if env.is_terminal((x, y)):\n",
    "            continue\n",
    "        action_values = np.zeros(len(env.A))\n",
    "        for a, action in enumerate(env.A):\n",
    "            next_state = env.next_state((x, y), action)\n",
    "            reward = env.R.get(next_state, 0)\n",
    "            action_values[a] = reward + gamma * V[next_state[0], next_state[1]]\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[x, y, best_action] = 1  # Use a deterministic policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "value_iteration(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FrameworkML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
