{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "from grid import GridWorldC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.60756262e-316              nan  6.93093377e-310              nan]\n",
      " [-1.00000000e+004              nan              nan              nan]\n",
      " [-1.00000000e+004              nan              nan              nan]\n",
      " [ 6.93093278e-310              nan              nan              nan]\n",
      " [ 6.48559973e-320              nan              nan  0.00000000e+000]\n",
      " [             nan              nan  0.00000000e+000              nan]\n",
      " [             nan              nan              nan              nan]\n",
      " [             nan              nan              nan              nan]\n",
      " [             nan              nan              nan              nan]\n",
      " [             nan              nan              nan  6.93079136e-310]\n",
      " [             nan  1.37491770e+270  6.93079136e-310              nan]\n",
      " [             nan  2.43727197e+306              nan              nan]\n",
      " [             nan              nan              nan              nan]\n",
      " [             nan  1.41700412e+258              nan              nan]\n",
      " [             nan              nan              nan  6.93079126e-310]\n",
      " [             nan  1.77440382e+029  6.93079136e-310  2.43908540e+206]\n",
      " [ 1.21869629e+302  2.46447687e+202 -2.41396525e+214              nan]\n",
      " [             nan -1.33793588e+150 -1.25007800e+152 -1.41672072e+262]\n",
      " [ 1.43131730e+254  1.22337638e+160              nan -1.86764594e+218]\n",
      " [             nan  9.87823202e+000  1.88708097e+210  6.93079136e-310]\n",
      " [ 6.93079136e-310  6.93079136e-310  6.93079136e-310  6.93079136e-310]\n",
      " [ 2.48937058e+198  6.93079126e-310 -2.99999982e+000 -1.24173134e+148]\n",
      " [-2.51426429e+198  0.00000000e+000 -1.24342570e+144  2.11964990e+157]\n",
      " [ 1.41700412e+258  0.00000000e+000 -2.03323233e+142  1.00044396e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]]\n",
      "Episode 1: Total Reward: 1, Exploration Rate: 0.995\n",
      "Episode 2: Total Reward: 1, Exploration Rate: 0.990025\n",
      "Episode 3: Total Reward: -3, Exploration Rate: 0.985075\n",
      "Episode 4: Total Reward: -3, Exploration Rate: 0.98015\n",
      "Episode 5: Total Reward: -3, Exploration Rate: 0.975249\n",
      "Episode 6: Total Reward: -3, Exploration Rate: 0.970373\n",
      "Episode 7: Total Reward: -3, Exploration Rate: 0.965521\n",
      "Episode 8: Total Reward: -3, Exploration Rate: 0.960693\n",
      "Episode 9: Total Reward: 1, Exploration Rate: 0.95589\n",
      "Episode 10: Total Reward: -1, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldC()\n",
    "q_learning = lib.QLearning(env, 10000)\n",
    "q_table = q_learning.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{22: -0.884391164860089, 14: -0.25181244111346496, 21: -1.7250789492412582, 23: -0.08332243504037862, 11: -1.0754960588978844, 12: -0.7465879995035719, 13: -0.4529053060444604, 8: -0.5115208112599433, 9: -0.40267500264917094, 3: -0.5647022506464578, 19: 0.14106901663907273, 6: -0.9080140122057399, 18: -0.30586592921997996, 5: -1.0474910922539884, 17: -0.8015599028821286, 4: -0.46959921129974946, 16: -1.3764472321502137, 15: -1.8916791163412408, 2: -0.6997603346180776, 10: -1.329739897215543, 1: -0.8281550137598502, 0: -0.9342009186714441, 7: -0.7032939142266301}\n"
     ]
    }
   ],
   "source": [
    "monte_carlo_es = lib.MonteCarloES(env, 10000)\n",
    "value_table = monte_carlo_es.run()\n",
    "print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{21: {2: -3.0}, 23: {3: 1.0}, 15: {1: -3.0}, 19: {1: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "mcoffpolicy = lib.OffPolicyMonteCarloControl(env, 10000)\n",
    "value_table = mcoffpolicy.run()\n",
    "print(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23: {0: 1.9175769524331162e-45}, 0: {1: -3.4230125604147836e-296, 3: -7.962543081828472e-296}, 1: {1: -8.0429727324492e-296, 2: -9.446119074643202e-296, 3: -7.795817644899245e-295}, 6: {0: -8.124214803000656e-296, 1: -2.9548684460849534e-290, 2: -2.9408708563613386e-295, 3: -1.397914672895735e-295}, 5: {0: -3.457588411556213e-296, 3: -4.391445339611191e-294, 1: -3.4193847609835133e-295}, 4: {1: -4.7540677913165533e-293, 2: -5.583444323331331e-293}, 2: {1: -1.4262979756793233e-295, 2: -2.2195325551366743e-295, 3: -1.4982136551356507e-294}, 9: {0: -4.802088631838653e-293, 1: -1.7382999895352764e-292, 2: -7.324058807197518e-293}, 3: {1: -8.115604544837222e-295, 3: -4.7065271587417115e-293, 2: -2.303239827235929e-292}, 7: {0: -1.4120350095247676e-295, 2: -1.4407050120602926e-295, 3: -7.954104167639906e-295, 1: -1.4998031863016195e-295}, 15: {0: -3.192811984580536e-164, 3: -2.47e-322, 1: -3.0}, 21: {0: -1.580808447025702e-125}, 8: {0: -8.034448576785287e-295, 2: -8.19758026935253e-295, 3: -6.1737665305006e-293, 1: -3.238037561083276e-291}, 16: {3: -9.952140871429789e-88, 2: -2.673933e-318, 0: -3.951807680122338e-204, 1: -1.5650003776312091e-125}, 17: {3: 1.7432951543824205e-129, 1: -1.1798229639837503e-20, 0: -8.475367137463425e-80, 2: -2.47e-322}, 18: {1: 1.898401201196224e-45, 2: 7.423753128467558e-159, 0: -1.6054088822367151e-217, 3: -2.5384245008197667e-117}, 11: {0: -2.984715573354028e-290, 1: -3.9122896410084904e-204, 2: -2.3391836845474594e-136, 3: -8.854864802186734e-254}, 19: {1: 1.0, 2: 0.0, 0: -1.8110721781793037e-292}, 14: {0: -1.7915095648864532e-292, 1: -1.7929614736692408e-292, 2: -1.2597038010309716e-121}, 13: {0: -3.2707449796879e-291, 3: -1.201454572487202e-228, 2: -2.5969050709492137e-263, 1: -1.5893548087247203e-217}, 12: {0: -1.514952698842328e-295, 3: -1.007167395022616e-128, 1: -8.39061354691619e-80, 2: -2.3303548727790416e-104}, 22: {0: -1.191740356180231e-20}, 10: {0: -3.453923967721543e-295, 1: -3.160883895183759e-164, 3: -3.909759139200575e-156}}\n"
     ]
    }
   ],
   "source": [
    "onpolicy_mc_control = lib.onPolicyMonteCarloControl(env, 10)\n",
    "q_table = onpolicy_mc_control.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: -3, Exploration Rate: 0.995\n",
      "Episode 2: Total Reward: 1, Exploration Rate: 0.990025\n",
      "Episode 3: Total Reward: -3, Exploration Rate: 0.985075\n",
      "Episode 4: Total Reward: -3, Exploration Rate: 0.98015\n",
      "Episode 5: Total Reward: 1, Exploration Rate: 0.975249\n",
      "Episode 6: Total Reward: 1, Exploration Rate: 0.970373\n",
      "Episode 7: Total Reward: -3, Exploration Rate: 0.965521\n",
      "Episode 8: Total Reward: 1, Exploration Rate: 0.960693\n",
      "Episode 9: Total Reward: 1, Exploration Rate: 0.95589\n",
      "[[ 2.60756262e-316  1.24549416e+140  6.93093377e-310  1.24380933e+140]\n",
      " [ 6.93093377e-310  1.24488502e+140  1.24587444e+140  1.24787825e+140]\n",
      " [ 6.93093377e-310  1.24770850e+140  1.24695432e+140  1.24799347e+140]\n",
      " [ 6.93093278e-310  1.25436087e+140  1.24587603e+140  1.24989204e+140]\n",
      " [ 6.48559973e-320  1.24462339e+140  1.25049995e+140  0.00000000e+000]\n",
      " [ 1.24430304e+140  1.24724841e+140  0.00000000e+000  1.24532301e+140]\n",
      " [ 1.24536243e+140  1.24771063e+140  1.24401838e+140  1.24935865e+140]\n",
      " [ 1.25131768e+140  1.25403738e+140  1.24543982e+140  1.25255864e+140]\n",
      " [ 1.25439267e+140  1.24928222e+140  1.25492348e+140  1.24796917e+140]\n",
      " [ 1.24458455e+140  1.24340410e+140  1.24902223e+140  6.93079136e-310]\n",
      " [ 1.24751581e+140  1.25106325e+140  6.93079136e-310  1.24652080e+140]\n",
      " [ 1.24702913e+140  1.24986303e+140  1.24921067e+140  1.25174083e+140]\n",
      " [ 1.25392820e+140  1.25374440e+140  1.24834230e+140  1.25490115e+140]\n",
      " [ 1.24906107e+140  1.25319362e+140  1.25344182e+140  1.24535917e+140]\n",
      " [ 1.24597958e+140  7.62713994e+123  1.24637981e+140  6.93079126e-310]\n",
      " [ 1.25098897e+140 -3.00000000e+000  6.93079136e-310  1.25185523e+140]\n",
      " [ 1.24921210e+140  1.24905628e+140  1.34787529e+117  1.25541809e+140]\n",
      " [ 1.25616625e+140  1.24746531e+140  1.25032940e+140  1.24558340e+140]\n",
      " [ 1.25071286e+140  1.24950145e+140  1.25076186e+140  7.62713994e+123]\n",
      " [ 7.70418176e+123  1.00000000e+000  8.17160837e+091  6.93079136e-310]\n",
      " [ 6.93079136e-310  6.93079136e-310  6.93079136e-310  6.93079136e-310]\n",
      " [ 1.24963636e+140  6.93079126e-310 -3.00000000e+000  1.62718947e+057]\n",
      " [ 1.25439952e+140  0.00000000e+000  1.24355006e+140  1.24627764e+140]\n",
      " [ 1.24804134e+140  0.00000000e+000  1.25157204e+140  1.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]]\n",
      "Episode 10: Total Reward: -3, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "dyna_Q = lib.DynaQ(env, 10000)\n",
    "q_table = dyna_Q.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.60756262e-316  1.43232822e+238  6.93093377e-310  1.46111808e+230]\n",
      " [ 6.93093377e-310 -1.44605828e+238  1.97009434e+223 -7.75932589e+213]\n",
      " [-1.00000000e+004  1.17589294e+206 -1.43188416e+234 -2.65091839e+218]\n",
      " [ 6.93093278e-310  1.14069845e+218  2.72411240e+210  2.67035901e+210]\n",
      " [ 6.48559973e-320 -1.52891793e+166 -1.12940443e+218  0.00000000e+000]\n",
      " [ 1.41814389e+234 -1.44606428e+246  0.00000000e+000 -1.41757109e+246]\n",
      " [-9.75099179e+230  1.46110930e+230  4.29524040e+246  1.49108296e+222]\n",
      " [-1.50546884e+222 -3.04242237e+214 -2.83514218e+246  1.54451599e+158]\n",
      " [ 1.12951737e+214 -3.08841111e+166  1.15260561e+206 -9.99330924e+189]\n",
      " [ 7.76284185e+201  1.04063911e+166  9.89386591e+189  6.93079136e-310]\n",
      " [ 1.40353573e+246 -1.22555051e+168  6.93079136e-310  1.46081706e+238]\n",
      " [-2.35670729e+192 -1.24456082e+202 -2.56518449e+194 -1.47557277e+230]\n",
      " [-1.49047751e+226 -3.02629488e+182  1.28252875e+194  5.14704595e+177]\n",
      " [-1.14119368e+206  2.18223069e+161  1.54404957e+166 -3.88278363e+189]\n",
      " [-7.93020667e+149 -7.62637723e+127 -1.16079544e+194  6.93079126e-310]\n",
      " [ 1.23817993e+160 -1.77529129e+009  6.93079136e-310 -1.27008445e+186]\n",
      " [ 1.28278530e+186 -1.24893137e+144 -1.34747097e+129 -1.25713214e+198]\n",
      " [ 2.53940693e+198 -1.33793588e+150 -1.25007800e+152 -5.09914227e+177]\n",
      " [ 5.15013369e+177 -1.22137887e+156  1.22598940e+152 -7.62562977e+135]\n",
      " [ 3.88239535e+193  1.00000000e+000  1.23820573e+144  6.93079136e-310]\n",
      " [ 6.93079136e-310  6.93079136e-310  6.93079136e-310  6.93079136e-310]\n",
      " [ 1.24963636e+140  6.93079126e-310 -3.00000000e+000  1.24185553e+144]\n",
      " [-2.51426429e+198  0.00000000e+000 -1.24342570e+144 -1.24615302e+144]\n",
      " [ 2.14106050e+153  0.00000000e+000 -2.03323233e+142  9.99999956e-001]\n",
      " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]]\n",
      "Episode 1: Total Reward: -3, Exploration Rate: 0.995\n",
      "Episode 2: Total Reward: -3, Exploration Rate: 0.990025\n",
      "Episode 3: Total Reward: -3, Exploration Rate: 0.985075\n",
      "Episode 4: Total Reward: 1, Exploration Rate: 0.98015\n",
      "Episode 5: Total Reward: -3, Exploration Rate: 0.975249\n",
      "Episode 6: Total Reward: -3, Exploration Rate: 0.970373\n",
      "Episode 7: Total Reward: 1, Exploration Rate: 0.965521\n",
      "Episode 8: Total Reward: -3, Exploration Rate: 0.960693\n",
      "Episode 9: Total Reward: -3, Exploration Rate: 0.95589\n",
      "Episode 10: Total Reward: -3, Exploration Rate: 0.95111\n"
     ]
    }
   ],
   "source": [
    "sarsa = lib.SARSA(env, 10000)\n",
    "q_table = sarsa.run()\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 0\n",
      "Delta: 1\n",
      "Delta: 1\n",
      "Delta: 1\n",
      "Delta: 1\n",
      "Delta: 1\n",
      "Delta: 1.99\n",
      "Delta: 1.99\n",
      "Delta: 1.99\n",
      "Delta: 1.99\n",
      "Delta: 1.99\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m policy_iteration \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mPolicyIteration(GridWorldC)\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Reset the environment before running the policy iteration\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m policy, V \u001b[38;5;241m=\u001b[39m policy_iteration\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(policy)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(V)\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:115\u001b[0m, in \u001b[0;36mGridWorldC.p\u001b[0;34m(self, s, a, s_p, r_index)\u001b[0m\n\u001b[1;32m    112\u001b[0m original_pos_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(s, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_with_state_return(a)\n\u001b[1;32m    116\u001b[0m next_state_id \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m next_state[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    118\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_state_id \u001b[38;5;241m==\u001b[39m s_p \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(r_index) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:170\u001b[0m, in \u001b[0;36mGridWorldC.step_with_state_return\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_with_state_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:159\u001b[0m, in \u001b[0;36mGridWorldC.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_game_over())\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "policy_iteration = lib.PolicyIteration(GridWorldC)\n",
    "env.reset()  # Reset the environment before running the policy iteration\n",
    "policy, V = policy_iteration.run()\n",
    "print(policy)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m value_iteration \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mValueIteration(GridWorldC)\n\u001b[0;32m----> 2\u001b[0m policy, V \u001b[38;5;241m=\u001b[39m value_iteration\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(policy)\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:115\u001b[0m, in \u001b[0;36mGridWorldC.p\u001b[0;34m(self, s, a, s_p, r_index)\u001b[0m\n\u001b[1;32m    112\u001b[0m original_pos_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(s, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_with_state_return(a)\n\u001b[1;32m    116\u001b[0m next_state_id \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m next_state[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    118\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_state_id \u001b[38;5;241m==\u001b[39m s_p \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(r_index) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:170\u001b[0m, in \u001b[0;36mGridWorldC.step_with_state_return\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_with_state_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_row\n",
      "File \u001b[0;32m/mnt/c/Users/User/OneDrive - Reseau-GES/Documents/ESGI/4eme année/Semestre 2/Deep Reinforcement Learning/notebook/Gridworld/grid.py:159\u001b[0m, in \u001b[0;36mGridWorldC.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_game_over())\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos_col \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "value_iteration = lib.ValueIteration(GridWorldC)\n",
    "policy, V = value_iteration.run()\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
